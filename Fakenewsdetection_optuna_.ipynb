{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarikasaxena30/Fake-News-Detection/blob/main/Fakenewsdetection_optuna_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_7tEz_UNXiW",
        "outputId": "03a7fb12-0e7f-486a-b94e-d833301b59fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch vers:  2.0.0+cu118\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "vers = torch.__version__\n",
        "print(\"Torch vers: \", vers)\n",
        "\n",
        "# PyG installation\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git\n",
        "\n",
        "import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nawGCOiiSEU9",
        "outputId": "83dbf4ec-8e62-48be-f325-2b68798219c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (6.0)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.10.3-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (2.0.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.10.3 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import UPFD\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader"
      ],
      "metadata": {
        "id": "T7SVmqHISIn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import ReLU, LeakyReLU, Softmax, Linear, SELU, GELU, ELU\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, TopKPooling,GCNConv"
      ],
      "metadata": {
        "id": "pwm4YDSKSMcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "G5eKCNkPSQDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\",split=\"test\")\n",
        "train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"train\")\n",
        "val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9534JfVxST1b",
        "outputId": "934e74b6-ede5-4198-8552-82f8f7daa5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://docs.google.com/uc?export=download&id=1VskhAQ92PrT4sWEKQ2v2-AJhEcpp4A81&confirm=t\n",
            "Extracting ./gossipcop/raw/uc\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data_gos, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(train_data_gos, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data_gos, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "oTuLkumhSVs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.leaky1 = LeakyReLU()\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.leaky2 = LeakyReLU()\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        self.leaky3 = LeakyReLU()\n",
        "\n",
        "        self.full1 = Linear(hidden_channels[2],hidden_channels[3])\n",
        "        self.ge1 = GELU()\n",
        "        self.full2 = Linear(hidden_channels[3],hidden_channels[4])\n",
        "        self.ge2 = GELU()\n",
        "        self.full3 = Linear(hidden_channels[4],out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.leaky1(self.conv1(x, edge_index))\n",
        "        h = self.leaky2(self.conv2(h, edge_index))\n",
        "        h = self.leaky3(self.conv3(h, edge_index))\n",
        "\n",
        "        h = global_max_pool(h,batch)\n",
        "\n",
        "        h = self.ge1(self.full1(h))\n",
        "        h = self.ge2(self.full2(h))\n",
        "        h = self.full3(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ],
      "metadata": {
        "id": "g0tEmqEvSY86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "XdB5W8lMSbQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  model = Net(768,[trial.suggest_int(name=\"layer_size1\",low=256,high=512,step=128),\n",
        "                   trial.suggest_int(name=\"layer_size2\",low=256,high=512,step=128),\n",
        "                   trial.suggest_int(name=\"layer_size3\",low=256,high=512,step=128),\n",
        "                   trial.suggest_int(name=\"layer_size4\",low=64,high=256,step=64),\n",
        "                   trial.suggest_int(name=\"layer_size5\",low=64,high=256,step=64),\n",
        "\n",
        "  ],1).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3),0.99))\n",
        "  lossff = torch.nn.BCELoss()\n",
        "\n",
        "  total_loss = 0\n",
        "  weighted_loss = 0\n",
        "  exp_param = 0.8\n",
        "\n",
        "  wloss = []\n",
        "\n",
        "  for i in range(400):\n",
        "    print(\"Epoch:\", i)\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "          data = data.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          out = model(data.x, data.edge_index, data.batch)\n",
        "          loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += float(loss) * data.num_graphs\n",
        "    print(\"Train: \",total_loss / len(train_loader.dataset))\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for data in val_loader:\n",
        "          data = data.to(device)\n",
        "          out = model(data.x, data.edge_index, data.batch)\n",
        "          loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "          total_loss += float(loss) * data.num_graphs\n",
        "\n",
        "    print(\"Test\", total_loss / len(val_loader.dataset))\n",
        "\n",
        "    weighted_loss = exp_param*(weighted_loss) + (1-exp_param)*(total_loss/ len(val_loader.dataset))\n",
        "    print(weighted_loss/(1-exp_param**(i+1)))\n",
        "    wloss.append(weighted_loss/(1-exp_param**(i+1)))\n",
        "\n",
        "    if(i-30>=0 and wloss[i-20]-weighted_loss<0.01):\n",
        "      break\n",
        "\n",
        "  return weighted_loss"
      ],
      "metadata": {
        "id": "E21fL2_pShuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(),pruner=optuna.pruners.HyperbandPruner())\n",
        "study.optimize(objective, n_trials=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jthzNJs0S2_F",
        "outputId": "c96e79d8-6b3b-4cb1-b9fb-956f9d3d4a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:23:45,712]\u001b[0m A new study created in memory with name: no-name-28f112cb-fd10-45a6-a7fc-76f451b7ad87\u001b[0m\n",
            "<ipython-input-10-543a82f29da4>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  optimizer = torch.optim.Adam(model.parameters(),lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3),0.99))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train:  0.6934147110352149\n",
            "Test 0.6920694894406385\n",
            "0.6920694894406385\n",
            "Epoch: 1\n",
            "Train:  1.038651126644987\n",
            "Test 0.6915197005638709\n",
            "0.6917640511757677\n",
            "Epoch: 2\n",
            "Train:  1.0375781332180176\n",
            "Test 0.6908056698875986\n",
            "0.691371271959305\n",
            "Epoch: 3\n",
            "Train:  1.036061392926471\n",
            "Test 0.6897645472606897\n",
            "0.6908269885248146\n",
            "Epoch: 4\n",
            "Train:  1.0343688453510131\n",
            "Test 0.6886534044594118\n",
            "0.6901803964063583\n",
            "Epoch: 5\n",
            "Train:  1.0319084457425407\n",
            "Test 0.6868456466730698\n",
            "0.6892764938201388\n",
            "Epoch: 6\n",
            "Train:  1.0287014187037289\n",
            "Test 0.6844488377972837\n",
            "0.6880547428708974\n",
            "Epoch: 7\n",
            "Train:  1.0244943370749227\n",
            "Test 0.6807808718838535\n",
            "0.6863066948875365\n",
            "Epoch: 8\n",
            "Train:  1.018651009479285\n",
            "Test 0.6764253906277946\n",
            "0.6840240645821329\n",
            "Epoch: 9\n",
            "Train:  1.011189103781522\n",
            "Test 0.6694483444804237\n",
            "0.6807582571599593\n",
            "Epoch: 10\n",
            "Train:  1.0005602401036482\n",
            "Test 0.6607464062861907\n",
            "0.6763797785352154\n",
            "Epoch: 11\n",
            "Train:  0.9863411884604792\n",
            "Test 0.6492130743278252\n",
            "0.6705455097990508\n",
            "Epoch: 12\n",
            "Train:  0.9680837333857358\n",
            "Test 0.634196305012965\n",
            "0.6628527551844126\n",
            "Epoch: 13\n",
            "Train:  0.9442594407242296\n",
            "Test 0.6139438390295148\n",
            "0.6526209734330954\n",
            "Epoch: 14\n",
            "Train:  0.9125992645929147\n",
            "Test 0.5879217683177291\n",
            "0.6392092493184186\n",
            "Epoch: 15\n",
            "Train:  0.8750805723798144\n",
            "Test 0.5579826818717705\n",
            "0.6224934272668217\n",
            "Epoch: 16\n",
            "Train:  0.8293828848517422\n",
            "Test 0.5238971943820353\n",
            "0.6023199135454733\n",
            "Epoch: 17\n",
            "Train:  0.7796108969401964\n",
            "Test 0.4886293644870157\n",
            "0.5791646760581026\n",
            "Epoch: 18\n",
            "Train:  0.7278624902933072\n",
            "Test 0.4495060922025324\n",
            "0.5528537793035662\n",
            "Epoch: 19\n",
            "Train:  0.6735718279749483\n",
            "Test 0.4130827356607486\n",
            "0.524573521400075\n",
            "Epoch: 20\n",
            "Train:  0.6206413348079164\n",
            "Test 0.37738608789967965\n",
            "0.49486199423113364\n",
            "Epoch: 21\n",
            "Train:  0.5716514921450353\n",
            "Test 0.3471167023146982\n",
            "0.4650932815179168\n",
            "Epoch: 22\n",
            "Train:  0.5297484075302606\n",
            "Test 0.3191507952117221\n",
            "0.43573146267017454\n",
            "Epoch: 23\n",
            "Train:  0.48929714119478024\n",
            "Test 0.29413017477744663\n",
            "0.40727683189698316\n",
            "Epoch: 24\n",
            "Train:  0.4549931418764722\n",
            "Test 0.2764740269699376\n",
            "0.38101706431455074\n",
            "Epoch: 25\n",
            "Train:  0.42770482987274616\n",
            "Test 0.2551909910016881\n",
            "0.3557755618919669\n",
            "Epoch: 26\n",
            "Train:  0.40078587595359744\n",
            "Test 0.23977982997894287\n",
            "0.332520187464181\n",
            "Epoch: 27\n",
            "Train:  0.3771820973345648\n",
            "Test 0.22562067630963448\n",
            "0.3110988503412099\n",
            "Epoch: 28\n",
            "Train:  0.35658397255363045\n",
            "Test 0.21696063196593587\n",
            "0.2922420271455956\n",
            "Epoch: 29\n",
            "Train:  0.34266977010792865\n",
            "Test 0.20380066133244135\n",
            "0.2745318298205925\n",
            "Epoch: 30\n",
            "Train:  0.32460538827346797\n",
            "Test 0.1958990088312617\n",
            "0.2587896753480654\n",
            "Epoch: 31\n",
            "Train:  0.31374845454544376\n",
            "Test 0.1875320057292561\n",
            "0.2445268412429196\n",
            "Epoch: 32\n",
            "Train:  0.30122014768285194\n",
            "Test 0.18200664536092745\n",
            "0.2320148716636476\n",
            "Epoch: 33\n",
            "Train:  0.2923584118785657\n",
            "Test 0.17857105140284305\n",
            "0.22132068501463684\n",
            "Epoch: 34\n",
            "Train:  0.28461659986239213\n",
            "Test 0.17187513361920367\n",
            "0.2114275616079263\n",
            "Epoch: 35\n",
            "Train:  0.27681719891099266\n",
            "Test 0.1684566861216402\n",
            "0.20283059663603056\n",
            "Epoch: 36\n",
            "Train:  0.2732046505524999\n",
            "Test 0.1698914722713468\n",
            "0.19624106102184033\n",
            "Epoch: 37\n",
            "Train:  0.26886909600047076\n",
            "Test 0.16328369277519184\n",
            "0.18964821809260546\n",
            "Epoch: 38\n",
            "Train:  0.2611485835396763\n",
            "Test 0.15994996593876198\n",
            "0.18370758060412895\n",
            "Epoch: 39\n",
            "Train:  0.257432037261707\n",
            "Test 0.15844336255784436\n",
            "0.17865406526746577\n",
            "Epoch: 40\n",
            "Train:  0.25391975113432924\n",
            "Test 0.1570890849292442\n",
            "0.1743406105146362\n",
            "Epoch: 41\n",
            "Train:  0.2527571323518761\n",
            "Test 0.15463438013316075\n",
            "0.17039902912567997\n",
            "Epoch: 42\n",
            "Train:  0.24761539902333374\n",
            "Test 0.1532159487788494\n",
            "0.16696217915642542\n",
            "Epoch: 43\n",
            "Train:  0.24445819716914233\n",
            "Test 0.1524055676582532\n",
            "0.16405069834069763\n",
            "Epoch: 44\n",
            "Train:  0.24163846404124528\n",
            "Test 0.15085198274462214\n",
            "0.16141084023944569\n",
            "Epoch: 45\n",
            "Train:  0.23945304084118907\n",
            "Test 0.15174669343909938\n",
            "0.159477943527756\n",
            "Epoch: 46\n",
            "Train:  0.24004352911488042\n",
            "Test 0.15313676190681946\n",
            "0.15820967184931423\n",
            "Epoch: 47\n",
            "Train:  0.23881927310001283\n",
            "Test 0.14855711538713057\n",
            "0.15627911750407694\n",
            "Epoch: 48\n",
            "Train:  0.2329972674558451\n",
            "Test 0.14724968666476862\n",
            "0.15447319911755467\n",
            "Epoch: 49\n",
            "Train:  0.23202597550815823\n",
            "Test 0.15017521949041457\n",
            "0.15361359092338855\n",
            "Epoch: 50\n",
            "Train:  0.2316992593035375\n",
            "Test 0.14658147145758618\n",
            "0.15220715097152265\n",
            "Epoch: 51\n",
            "Train:  0.22909401672390792\n",
            "Test 0.1456243999425199\n",
            "0.15089058873973552\n",
            "Epoch: 52\n",
            "Train:  0.22660728337604813\n",
            "Test 0.14426345710434538\n",
            "0.14956515272702298\n",
            "Epoch: 53\n",
            "Train:  0.2250436781683848\n",
            "Test 0.1439927472654498\n",
            "0.1484506651194065\n",
            "Epoch: 54\n",
            "Train:  0.2238023147710186\n",
            "Test 0.14551610953341693\n",
            "0.14786375125732673\n",
            "Epoch: 55\n",
            "Train:  0.22308927477293072\n",
            "Test 0.1430788090002242\n",
            "0.14690675922537397\n",
            "Epoch: 56\n",
            "Train:  0.22035476989232194\n",
            "Test 0.14237899462873246\n",
            "0.14600120359557697\n",
            "Epoch: 57\n",
            "Train:  0.2193100927499952\n",
            "Test 0.14501348981163004\n",
            "0.14580366036576553\n",
            "Epoch: 58\n",
            "Train:  0.21913545180262226\n",
            "Test 0.14233547896692605\n",
            "0.14511002275725193\n",
            "Epoch: 59\n",
            "Train:  0.21717337256590852\n",
            "Test 0.14363859397369427\n",
            "0.14481573654954813\n",
            "Epoch: 60\n",
            "Train:  0.2179493539226361\n",
            "Test 0.1470456434065824\n",
            "0.14526171846772723\n",
            "Epoch: 61\n",
            "Train:  0.21845778094897994\n",
            "Test 0.13958732279583866\n",
            "0.14412683822026218\n",
            "Epoch: 62\n",
            "Train:  0.21262736674125268\n",
            "Test 0.1390978223456568\n",
            "0.14312103425614936\n",
            "Epoch: 63\n",
            "Train:  0.21054132075318488\n",
            "Test 0.1432502784735554\n",
            "0.14314688311585616\n",
            "Epoch: 64\n",
            "Train:  0.21512933346978474\n",
            "Test 0.14079733131514802\n",
            "0.14267697251974043\n",
            "Epoch: 65\n",
            "Train:  0.21123301632866098\n",
            "Test 0.1377175264299298\n",
            "0.14168508290330203\n",
            "Epoch: 66\n",
            "Train:  0.20763498154936202\n",
            "Test 0.13744196380341883\n",
            "0.14083645881058812\n",
            "Epoch: 67\n",
            "Train:  0.20567464811434022\n",
            "Test 0.1378165136906745\n",
            "0.14023246963131367\n",
            "Epoch: 68\n",
            "Train:  0.20479245760372325\n",
            "Test 0.13634196851716374\n",
            "0.1394543692484377\n",
            "Epoch: 69\n",
            "Train:  0.20393664769200615\n",
            "Test 0.13954619271851285\n",
            "0.13947273394547466\n",
            "Epoch: 70\n",
            "Train:  0.2052847771144612\n",
            "Test 0.1356074027307741\n",
            "0.13869966760076782\n",
            "Epoch: 71\n",
            "Train:  0.20119513464143207\n",
            "Test 0.13535669697073352\n",
            "0.13803107340434978\n",
            "Epoch: 72\n",
            "Train:  0.20052558453855934\n",
            "Test 0.13666195081267166\n",
            "0.13775724886294446\n",
            "Epoch: 73\n",
            "Train:  0.20082745538007862\n",
            "Test 0.13478519973573666\n",
            "0.13716283899743978\n",
            "Epoch: 74\n",
            "Train:  0.1974335275249276\n",
            "Test 0.1339432439142531\n",
            "0.1365189199460824\n",
            "Epoch: 75\n",
            "Train:  0.19510992074678668\n",
            "Test 0.1343242956219657\n",
            "0.1360799950623256\n",
            "Epoch: 76\n",
            "Train:  0.19443425563703062\n",
            "Test 0.13450852694201382\n",
            "0.13576570142741737\n",
            "Epoch: 77\n",
            "Train:  0.19540123208920598\n",
            "Test 0.1327172673610977\n",
            "0.13515601459732182\n",
            "Epoch: 78\n",
            "Train:  0.19096923084595266\n",
            "Test 0.13395925747715073\n",
            "0.1349166631680014\n",
            "Epoch: 79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:24:31,952]\u001b[0m Trial 0 finished with value: 0.1359454151202222 and parameters: {'layer_size1': 512, 'layer_size2': 384, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 64, 'learning_rate': 8.725039194497957e-06, 'b1': 0.9161235464257066}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.19229142748556294\n",
            "Test 0.140060434847961\n",
            "0.13594541752216982\n",
            "Epoch: 0\n",
            "Train:  0.6836875290661067\n",
            "Test 0.5594880904033507\n",
            "0.5594880904033507\n",
            "Epoch: 1\n",
            "Train:  0.7037098809476301\n",
            "Test 0.23874806948415525\n",
            "0.38129918989268663\n",
            "Epoch: 2\n",
            "Train:  0.3325709589780905\n",
            "Test 0.15170147124807531\n",
            "0.28720176421866556\n",
            "Epoch: 3\n",
            "Train:  0.2412189018406015\n",
            "Test 0.1435642197946489\n",
            "0.2385440594679824\n",
            "Epoch: 4\n",
            "Train:  0.22041506393925175\n",
            "Test 0.14968347809731197\n",
            "0.2121100454952699\n",
            "Epoch: 5\n",
            "Train:  0.20094118520255871\n",
            "Test 0.19106734428391026\n",
            "0.20640630351543654\n",
            "Epoch: 6\n",
            "Train:  0.26404970005024975\n",
            "Test 0.22499165857285808\n",
            "0.21110976106831408\n",
            "Epoch: 7\n",
            "Train:  0.2666235258222827\n",
            "Test 0.1560648519191004\n",
            "0.19788143428001204\n",
            "Epoch: 8\n",
            "Train:  0.20198687840075719\n",
            "Test 0.1416940406400236\n",
            "0.18490186760207747\n",
            "Epoch: 9\n",
            "Train:  0.1686488956089546\n",
            "Test 0.15271021485765338\n",
            "0.17768906866444925\n",
            "Epoch: 10\n",
            "Train:  0.1518862579536416\n",
            "Test 0.1666542769720157\n",
            "0.17527471929423913\n",
            "Epoch: 11\n",
            "Train:  0.14222157013435394\n",
            "Test 0.17823373706117546\n",
            "0.1759101922141635\n",
            "Epoch: 12\n",
            "Train:  0.13336003403394267\n",
            "Test 0.2138437180278393\n",
            "0.18393824420236982\n",
            "Epoch: 13\n",
            "Train:  0.14643021935815395\n",
            "Test 0.31848777268395756\n",
            "0.21208610595023997\n",
            "Epoch: 14\n",
            "Train:  0.20926925911134853\n",
            "Test 0.1877816497372129\n",
            "0.20704795036695067\n",
            "Epoch: 15\n",
            "Train:  0.12523115406472973\n",
            "Test 0.20482793980365066\n",
            "0.2065910887427886\n",
            "Epoch: 16\n",
            "Train:  0.12389387274540503\n",
            "Test 0.2202401565449251\n",
            "0.209383788306795\n",
            "Epoch: 17\n",
            "Train:  0.14994200133469515\n",
            "Test 0.30360693746031475\n",
            "0.22857412042785707\n",
            "Epoch: 18\n",
            "Train:  0.1996652799301982\n",
            "Test 0.21078742300407557\n",
            "0.2249647646443434\n",
            "Epoch: 19\n",
            "Train:  0.13762053242385014\n",
            "Test 0.20243992699568067\n",
            "0.220407252577922\n",
            "Epoch: 20\n",
            "Train:  0.12136352871475738\n",
            "Test 0.32183066465219845\n",
            "0.2408807698595689\n",
            "Epoch: 21\n",
            "Train:  0.18909725462702262\n",
            "Test 0.22818382245389532\n",
            "0.2383225037060315\n",
            "Epoch: 22\n",
            "Train:  0.12037402759954306\n",
            "Test 0.27494341817312623\n",
            "0.2456901776701817\n",
            "Epoch: 23\n",
            "Train:  0.14350920021577843\n",
            "Test 0.36427722308418786\n",
            "0.2695201204761786\n",
            "Epoch: 24\n",
            "Train:  0.19868788334364024\n",
            "Test 0.388418199828802\n",
            "0.2933899138765489\n",
            "Epoch: 25\n",
            "Train:  0.22531572053564589\n",
            "Test 0.2983360092504195\n",
            "0.2943821317458251\n",
            "Epoch: 26\n",
            "Train:  0.16176616207638908\n",
            "Test 0.2629474911414952\n",
            "0.28807996592299373\n",
            "Epoch: 27\n",
            "Train:  0.15193220991531212\n",
            "Test 0.27983870506675984\n",
            "0.2864285193895806\n",
            "Epoch: 28\n",
            "Train:  0.1483731558667489\n",
            "Test 0.31036634017259646\n",
            "0.29122350342458964\n",
            "Epoch: 29\n",
            "Train:  0.1603991995296503\n",
            "Test 0.33231740282130745\n",
            "0.2994524702714749\n",
            "Epoch: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:24:47,405]\u001b[0m Trial 1 finished with value: 0.29991363899795165 and parameters: {'layer_size1': 384, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 64, 'learning_rate': 0.0006065539019270989, 'b1': 0.9483548996636576}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.18420045612693095\n",
            "Test 0.30324113071510644\n",
            "0.30021095352563276\n",
            "Epoch: 0\n",
            "Train:  0.6928075822718415\n",
            "Test 0.693823007000235\n",
            "0.693823007000235\n",
            "Epoch: 1\n",
            "Train:  1.03961358271239\n",
            "Test 0.6937528492766859\n",
            "0.6937840304871523\n",
            "Epoch: 2\n",
            "Train:  1.039482941666802\n",
            "Test 0.6936577700433277\n",
            "0.6937322844036176\n",
            "Epoch: 3\n",
            "Train:  1.039337926186048\n",
            "Test 0.6935703817741338\n",
            "0.6936774393394293\n",
            "Epoch: 4\n",
            "Train:  1.0392057385855105\n",
            "Test 0.6935066615705525\n",
            "0.6936266368141804\n",
            "Epoch: 5\n",
            "Train:  1.0390783957727663\n",
            "Test 0.6934289360221052\n",
            "0.6935730489075768\n",
            "Epoch: 6\n",
            "Train:  1.0389466628486856\n",
            "Test 0.6933505679224874\n",
            "0.6935167449055032\n",
            "Epoch: 7\n",
            "Train:  1.038811811379024\n",
            "Test 0.6932797654644474\n",
            "0.6934597942894181\n",
            "Epoch: 8\n",
            "Train:  1.0386787196655414\n",
            "Test 0.6932061135550558\n",
            "0.6934011927812638\n",
            "Epoch: 9\n",
            "Train:  1.0385369205431187\n",
            "Test 0.6931110312650491\n",
            "0.6933361797577787\n",
            "Epoch: 10\n",
            "Train:  1.0383841477252624\n",
            "Test 0.6929778725236326\n",
            "0.6932577841822922\n",
            "Epoch: 11\n",
            "Train:  1.038204937845796\n",
            "Test 0.6928773229812091\n",
            "0.6931760770722263\n",
            "Epoch: 12\n",
            "Train:  1.0380303441604852\n",
            "Test 0.6927575484300271\n",
            "0.6930875018705792\n",
            "Epoch: 13\n",
            "Train:  1.037854566242232\n",
            "Test 0.692606447380541\n",
            "0.692986864912341\n",
            "Epoch: 14\n",
            "Train:  1.0376532828851497\n",
            "Test 0.692475581343794\n",
            "0.6928808791563569\n",
            "Epoch: 15\n",
            "Train:  1.0374568886372633\n",
            "Test 0.6923760330720699\n",
            "0.6927769855957441\n",
            "Epoch: 16\n",
            "Train:  1.0372859655500768\n",
            "Test 0.6922794445093735\n",
            "0.6926751850336013\n",
            "Epoch: 17\n",
            "Train:  1.0371207736787342\n",
            "Test 0.6921721039674221\n",
            "0.6925727230289822\n",
            "Epoch: 18\n",
            "Train:  1.0369010274008517\n",
            "Test 0.6920282478297587\n",
            "0.6924622356989021\n",
            "Epoch: 19\n",
            "Train:  1.036681090737437\n",
            "Test 0.691890347134936\n",
            "0.692346523920128\n",
            "Epoch: 20\n",
            "Train:  1.0364591627767235\n",
            "Test 0.6917145293710869\n",
            "0.692218948333219\n",
            "Epoch: 21\n",
            "Train:  1.036204830521629\n",
            "Test 0.6915517881676391\n",
            "0.6920845244267428\n",
            "Epoch: 22\n",
            "Train:  1.03595205617475\n",
            "Test 0.6913770762555328\n",
            "0.6919421946256481\n",
            "Epoch: 23\n",
            "Train:  1.0356910256020753\n",
            "Test 0.6912101210255326\n",
            "0.6917950852010087\n",
            "Epoch: 24\n",
            "Train:  1.03542840218806\n",
            "Test 0.6910202529404189\n",
            "0.6916395310820447\n",
            "Epoch: 25\n",
            "Train:  1.0351550823602922\n",
            "Test 0.6908708564091078\n",
            "0.6914853301035967\n",
            "Epoch: 26\n",
            "Train:  1.0348863792725098\n",
            "Test 0.6906728718306993\n",
            "0.6913224446160742\n",
            "Epoch: 27\n",
            "Train:  1.0345848641552768\n",
            "Test 0.6904871979039231\n",
            "0.6911550715270056\n",
            "Epoch: 28\n",
            "Train:  1.0342949914844919\n",
            "Test 0.6902892209671356\n",
            "0.69098163303196\n",
            "Epoch: 29\n",
            "Train:  1.033973168322455\n",
            "Test 0.690070133069496\n",
            "0.6907991070832871\n",
            "Epoch: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:25:03,365]\u001b[0m Trial 2 finished with value: 0.6899274929819494 and parameters: {'layer_size1': 512, 'layer_size2': 384, 'layer_size3': 384, 'layer_size4': 64, 'layer_size5': 256, 'learning_rate': 1.3743331730078e-06, 'b1': 0.9696270041516911}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  1.0336395486807213\n",
            "Test 0.6898617080716423\n",
            "0.6906114414258931\n",
            "Epoch: 0\n",
            "Train:  0.6932247676255502\n",
            "Test 0.6928401517344045\n",
            "0.6928401517344045\n",
            "Epoch: 1\n",
            "Train:  1.039460423010173\n",
            "Test 0.6927529062543597\n",
            "0.6927916820232686\n",
            "Epoch: 2\n",
            "Train:  1.0392349438789563\n",
            "Test 0.6926388487274393\n",
            "0.6927290454266172\n",
            "Epoch: 3\n",
            "Train:  1.0390347587319957\n",
            "Test 0.6925341421431237\n",
            "0.6926630212790923\n",
            "Epoch: 4\n",
            "Train:  1.038826055688299\n",
            "Test 0.6924136802827046\n",
            "0.6925888479698384\n",
            "Epoch: 5\n",
            "Train:  1.0385917850903101\n",
            "Test 0.6922635457891247\n",
            "0.6925006729924138\n",
            "Epoch: 6\n",
            "Train:  1.0383611010326135\n",
            "Test 0.692111937335996\n",
            "0.6924022943682979\n",
            "Epoch: 7\n",
            "Train:  1.0381105466858371\n",
            "Test 0.6919612329958122\n",
            "0.6922962990263287\n",
            "Epoch: 8\n",
            "Train:  1.0378444039777959\n",
            "Test 0.6918016565151703\n",
            "0.692182034152317\n",
            "Epoch: 9\n",
            "Train:  1.037576635351111\n",
            "Test 0.691638822957273\n",
            "0.6920603233112606\n",
            "Epoch: 10\n",
            "Train:  1.037291006727533\n",
            "Test 0.6914657546487046\n",
            "0.6919302350852207\n",
            "Epoch: 11\n",
            "Train:  1.036974304979974\n",
            "Test 0.6912497306044721\n",
            "0.6917840912620118\n",
            "Epoch: 12\n",
            "Train:  1.0366272257142888\n",
            "Test 0.6910341873273744\n",
            "0.6916253855354996\n",
            "Epoch: 13\n",
            "Train:  1.0362456838289897\n",
            "Test 0.6907711983163715\n",
            "0.6914466889319354\n",
            "Epoch: 14\n",
            "Train:  1.0358248740543812\n",
            "Test 0.690512681837047\n",
            "0.6912530753403084\n",
            "Epoch: 15\n",
            "Train:  1.0353954086155246\n",
            "Test 0.6902351075913007\n",
            "0.6910435851663225\n",
            "Epoch: 16\n",
            "Train:  1.0349557667206495\n",
            "Test 0.6899347038932773\n",
            "0.6908166999099328\n",
            "Epoch: 17\n",
            "Train:  1.0344943971860976\n",
            "Test 0.6896490323674548\n",
            "0.6905788822595099\n",
            "Epoch: 18\n",
            "Train:  1.0339885204484611\n",
            "Test 0.689320676492684\n",
            "0.6903235615468929\n",
            "Epoch: 19\n",
            "Train:  1.033472601092342\n",
            "Test 0.6890029743477538\n",
            "0.6900563635235947\n",
            "Epoch: 20\n",
            "Train:  1.0328330396514236\n",
            "Test 0.6886022167328076\n",
            "0.6897628267667221\n",
            "Epoch: 21\n",
            "Train:  1.0321961024523654\n",
            "Test 0.6882171028262967\n",
            "0.6894513839361613\n",
            "Epoch: 22\n",
            "Train:  1.0315067222266843\n",
            "Test 0.6877502425686344\n",
            "0.6891091353837904\n",
            "Epoch: 23\n",
            "Train:  1.030751143401359\n",
            "Test 0.6872521359405238\n",
            "0.6887359732869582\n",
            "Epoch: 24\n",
            "Train:  1.0299401848744123\n",
            "Test 0.686652191611\n",
            "0.6883176365201457\n",
            "Epoch: 25\n",
            "Train:  1.0290251141721076\n",
            "Test 0.6860780436477382\n",
            "0.6878683600909757\n",
            "Epoch: 26\n",
            "Train:  1.0280957964313773\n",
            "Test 0.6854328513145447\n",
            "0.6873800777414103\n",
            "Epoch: 27\n",
            "Train:  1.0270401674967546\n",
            "Test 0.6847560562930264\n",
            "0.6868542563652725\n",
            "Epoch: 28\n",
            "Train:  1.0259556735391582\n",
            "Test 0.6840148011406699\n",
            "0.6862854851895877\n",
            "Epoch: 29\n",
            "Train:  1.0248128158268912\n",
            "Test 0.6831702843253866\n",
            "0.6856616727743816\n",
            "Epoch: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:25:19,855]\u001b[0m Trial 3 finished with value: 0.6843034620064379 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 384, 'layer_size4': 128, 'layer_size5': 192, 'learning_rate': 2.0562394015838844e-06, 'b1': 0.9700961723424958}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  1.0235480820084666\n",
            "Test 0.682265851087186\n",
            "0.6849818351583784\n",
            "Epoch: 0\n",
            "Train:  0.6874305324240045\n",
            "Test 0.6713089506268065\n",
            "0.6713089506268065\n",
            "Epoch: 1\n",
            "Train:  0.8455676718072577\n",
            "Test 0.19948303142746726\n",
            "0.409183439960507\n",
            "Epoch: 2\n",
            "Train:  0.29976455122232437\n",
            "Test 0.1480269523502938\n",
            "0.30215209257927206\n",
            "Epoch: 3\n",
            "Train:  0.24455415524393934\n",
            "Test 0.2177872383632721\n",
            "0.27357321242480054\n",
            "Epoch: 4\n",
            "Train:  0.322779328934166\n",
            "Test 0.15617502819159965\n",
            "0.23864990678665177\n",
            "Epoch: 5\n",
            "Train:  0.23730770330671425\n",
            "Test 0.1430241364180605\n",
            "0.21273000632678124\n",
            "Epoch: 6\n",
            "Train:  0.21677920457671632\n",
            "Test 0.1647471681837634\n",
            "0.20058683005842384\n",
            "Epoch: 7\n",
            "Train:  0.24269465119629116\n",
            "Test 0.18280196279942335\n",
            "0.19631279201158308\n",
            "Epoch: 8\n",
            "Train:  0.24923140319548684\n",
            "Test 0.13352901832415506\n",
            "0.1818094288178798\n",
            "Epoch: 9\n",
            "Train:  0.19623898319736288\n",
            "Test 0.13485113618001615\n",
            "0.17128804534185169\n",
            "Epoch: 10\n",
            "Train:  0.18271537446126943\n",
            "Test 0.12460197064165886\n",
            "0.16107339895869596\n",
            "Epoch: 11\n",
            "Train:  0.16647109806865126\n",
            "Test 0.12827784738643266\n",
            "0.15403028987330034\n",
            "Epoch: 12\n",
            "Train:  0.16083097069664098\n",
            "Test 0.13241537942827403\n",
            "0.1494558238554187\n",
            "Epoch: 13\n",
            "Train:  0.15070249439104572\n",
            "Test 0.1199846149084496\n",
            "0.1432904249547797\n",
            "Epoch: 14\n",
            "Train:  0.12864846802886315\n",
            "Test 0.11941690282425382\n",
            "0.1383415992019854\n",
            "Epoch: 15\n",
            "Train:  0.12015399354895502\n",
            "Test 0.12597521177182594\n",
            "0.13579668885797233\n",
            "Epoch: 16\n",
            "Train:  0.11701059428550262\n",
            "Test 0.1441006734939514\n",
            "0.13749574513116222\n",
            "Epoch: 17\n",
            "Train:  0.13760005522230656\n",
            "Test 0.15232377581301762\n",
            "0.14051575492750723\n",
            "Epoch: 18\n",
            "Train:  0.12979548889333417\n",
            "Test 0.12645339458198337\n",
            "0.13766215819542882\n",
            "Epoch: 19\n",
            "Train:  0.09340997300649764\n",
            "Test 0.13635298871209645\n",
            "0.13739727034982732\n",
            "Epoch: 20\n",
            "Train:  0.0975498412230974\n",
            "Test 0.20078970685264036\n",
            "0.15019378466267222\n",
            "Epoch: 21\n",
            "Train:  0.11705674832975314\n",
            "Test 0.15326058777026155\n",
            "0.15081170472952257\n",
            "Epoch: 22\n",
            "Train:  0.0866005093355206\n",
            "Test 0.16994434626717803\n",
            "0.1546609550001313\n",
            "Epoch: 23\n",
            "Train:  0.09591154577455009\n",
            "Test 0.1831973091862399\n",
            "0.16039530554215367\n",
            "Epoch: 24\n",
            "Train:  0.09990732457548795\n",
            "Test 0.18225978450301983\n",
            "0.16478478431627908\n",
            "Epoch: 25\n",
            "Train:  0.09350027759118458\n",
            "Test 0.19290039106016302\n",
            "0.1704249520259839\n",
            "Epoch: 26\n",
            "Train:  0.09720339267473735\n",
            "Test 0.2033330560509511\n",
            "0.17702252478308303\n",
            "Epoch: 27\n",
            "Train:  0.10200218733694359\n",
            "Test 0.21845400986628288\n",
            "0.1853248808919849\n",
            "Epoch: 28\n",
            "Train:  0.10947845358256574\n",
            "Test 0.22383940685177972\n",
            "0.19303972422576848\n",
            "Epoch: 29\n",
            "Train:  0.11211363970313878\n",
            "Test 0.22716859002610573\n",
            "0.19987395775716224\n",
            "Epoch: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:25:35,892]\u001b[0m Trial 4 finished with value: 0.20655572700232835 and parameters: {'layer_size1': 384, 'layer_size2': 384, 'layer_size3': 512, 'layer_size4': 192, 'layer_size5': 192, 'learning_rate': 0.00011859596906310394, 'b1': 0.9309428561859905}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.1137260102315973\n",
            "Test 0.23427253188346472\n",
            "0.20676049267626942\n",
            "Epoch: 0\n",
            "Train:  0.6913343506418305\n",
            "Test 0.6864568781066727\n",
            "0.6864568781066727\n",
            "Epoch: 1\n",
            "Train:  1.0173049759078812\n",
            "Test 0.6446003614764511\n",
            "0.6632032577565496\n",
            "Epoch: 2\n",
            "Train:  0.9197227830415243\n",
            "Test 0.5062695161981897\n",
            "0.5988861505605004\n",
            "Epoch: 3\n",
            "Train:  0.6876715212623715\n",
            "Test 0.3360540930168096\n",
            "0.5098509007150768\n",
            "Epoch: 4\n",
            "Train:  0.47991113315571793\n",
            "Test 0.23108510187257342\n",
            "0.42692437797516025\n",
            "Epoch: 5\n",
            "Train:  0.38184085417361485\n",
            "Test 0.1967802976732289\n",
            "0.36454253645000323\n",
            "Epoch: 6\n",
            "Train:  0.3313932577523338\n",
            "Test 0.1712149727966759\n",
            "0.31561648759943006\n",
            "Epoch: 7\n",
            "Train:  0.2869605522862066\n",
            "Test 0.15449971535273796\n",
            "0.27689709742697627\n",
            "Epoch: 8\n",
            "Train:  0.26145839128530696\n",
            "Test 0.15814130061439105\n",
            "0.24946391921272307\n",
            "Epoch: 9\n",
            "Train:  0.277048452399098\n",
            "Test 0.20272935585088128\n",
            "0.23899266408199052\n",
            "Epoch: 10\n",
            "Train:  0.30422961450908537\n",
            "Test 0.17676972285349726\n",
            "0.2253786400791048\n",
            "Epoch: 11\n",
            "Train:  0.30255667729501107\n",
            "Test 0.16286790103484422\n",
            "0.2119539549315684\n",
            "Epoch: 12\n",
            "Train:  0.3138539818125767\n",
            "Test 0.2083788674201939\n",
            "0.21119734220475053\n",
            "Epoch: 13\n",
            "Train:  0.27908523389626116\n",
            "Test 0.21464362456343877\n",
            "0.21191830704551365\n",
            "Epoch: 14\n",
            "Train:  0.27392284104245757\n",
            "Test 0.163185806521948\n",
            "0.20181637687071502\n",
            "Epoch: 15\n",
            "Train:  0.277735178406613\n",
            "Test 0.19016862585466873\n",
            "0.19941935654342588\n",
            "Epoch: 16\n",
            "Train:  0.24309168755261895\n",
            "Test 0.17271910504107074\n",
            "0.19395628889584512\n",
            "Epoch: 17\n",
            "Train:  0.2674777928616974\n",
            "Test 0.14939567495833386\n",
            "0.18488067437158442\n",
            "Epoch: 18\n",
            "Train:  0.22074223326130227\n",
            "Test 0.16957092854184108\n",
            "0.18177395262678056\n",
            "Epoch: 19\n",
            "Train:  0.2513433241732282\n",
            "Test 0.15589438674241413\n",
            "0.1765376692123805\n",
            "Epoch: 20\n",
            "Train:  0.2137036519398019\n",
            "Test 0.1450845121573179\n",
            "0.17018847683796517\n",
            "Epoch: 21\n",
            "Train:  0.22655927198819625\n",
            "Test 0.15245949164810743\n",
            "0.16661632194918363\n",
            "Epoch: 22\n",
            "Train:  0.2012336420529819\n",
            "Test 0.1383343935797448\n",
            "0.16092634860101104\n",
            "Epoch: 23\n",
            "Train:  0.21563973151413457\n",
            "Test 0.16848722941518485\n",
            "0.16244569969653502\n",
            "Epoch: 24\n",
            "Train:  0.20947450923736824\n",
            "Test 0.12547766283195425\n",
            "0.15502405413943762\n",
            "Epoch: 25\n",
            "Train:  0.1829358898359095\n",
            "Test 0.16433116427926353\n",
            "0.1568911190247697\n",
            "Epoch: 26\n",
            "Train:  0.20731777108646246\n",
            "Test 0.12363017385914213\n",
            "0.15022280700249024\n",
            "Epoch: 27\n",
            "Train:  0.17400576064410883\n",
            "Test 0.15850442412246238\n",
            "0.151882340430981\n",
            "Epoch: 28\n",
            "Train:  0.19345701010509705\n",
            "Test 0.13131934686840235\n",
            "0.14776336791722092\n",
            "Epoch: 29\n",
            "Train:  0.16020751628519853\n",
            "Test 0.14319462151072182\n",
            "0.14684848606704753\n",
            "Epoch: 30\n",
            "Train:  0.17919158681394745\n",
            "Test 0.1298557488800405\n",
            "0.14344656953469756\n",
            "Epoch: 31\n",
            "Train:  0.14681623927948675\n",
            "Test 0.1291037332607713\n",
            "0.1405757277647293\n",
            "Epoch: 32\n",
            "Train:  0.15609861814822906\n",
            "Test 0.1297195587893982\n",
            "0.13840311691393853\n",
            "Epoch: 33\n",
            "Train:  0.13880743895531136\n",
            "Test 0.13325793043311154\n",
            "0.13737355756916456\n",
            "Epoch: 34\n",
            "Train:  0.14891268734584798\n",
            "Test 0.13126764910452532\n",
            "0.13615188030506323\n",
            "Epoch: 35\n",
            "Train:  0.1288473501301715\n",
            "Test 0.12413017412702188\n",
            "0.1337467585628258\n",
            "Epoch: 36\n",
            "Train:  0.1333868812322753\n",
            "Test 0.12786268126970027\n",
            "0.13256963750610246\n",
            "Epoch: 37\n",
            "Train:  0.12160592025910051\n",
            "Test 0.13268843527896937\n",
            "0.13259340199636738\n",
            "Epoch: 38\n",
            "Train:  0.12771256611897394\n",
            "Test 0.13326086464164022\n",
            "0.13272691670935874\n",
            "Epoch: 39\n",
            "Train:  0.11610966200356955\n",
            "Test 0.12738129275555862\n",
            "0.1316576497886461\n",
            "Epoch: 40\n",
            "Train:  0.11791614144243089\n",
            "Test 0.12460576401743696\n",
            "0.13024712264143046\n",
            "Epoch: 41\n",
            "Train:  0.10519038081841181\n",
            "Test 0.1409881772761721\n",
            "0.13239551633350144\n",
            "Epoch: 42\n",
            "Train:  0.11724845704156905\n",
            "Test 0.13840824828206838\n",
            "0.1335981445698514\n",
            "Epoch: 43\n",
            "Train:  0.10908496485606874\n",
            "Test 0.12860801521920678\n",
            "0.13260006435906702\n",
            "Epoch: 44\n",
            "Train:  0.10364157079135651\n",
            "Test 0.12271145007985852\n",
            "0.1306222553574937\n",
            "Epoch: 45\n",
            "Train:  0.09475486832559996\n",
            "Test 0.1451387342919285\n",
            "0.13352565231299895\n",
            "Epoch: 46\n",
            "Train:  0.10636522565587106\n",
            "Test 0.1403233148566969\n",
            "0.13488522272103012\n",
            "Epoch: 47\n",
            "Train:  0.09583624963215365\n",
            "Test 0.13188294891691033\n",
            "0.13428475456931893\n",
            "Epoch: 48\n",
            "Train:  0.0914665107578406\n",
            "Test 0.1299714206994235\n",
            "0.1334220724045757\n",
            "Epoch: 49\n",
            "Train:  0.08230310822828527\n",
            "Test 0.15027747525161494\n",
            "0.13679320108833992\n",
            "Epoch: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:26:01,632]\u001b[0m Trial 5 finished with value: 0.1417852554843158 and parameters: {'layer_size1': 384, 'layer_size2': 512, 'layer_size3': 512, 'layer_size4': 64, 'layer_size5': 128, 'learning_rate': 0.00010230781005970434, 'b1': 0.9969943668250193}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.09315538719328864\n",
            "Test 0.16176128257944455\n",
            "0.1417868744042306\n",
            "Epoch: 0\n",
            "Train:  0.6928197894340906\n",
            "Test 0.6938781480649452\n",
            "0.6938781480649452\n",
            "Epoch: 1\n",
            "Train:  1.039179415711553\n",
            "Test 0.6930755328782748\n",
            "0.6934322507390173\n",
            "Epoch: 2\n",
            "Train:  1.0380691955814432\n",
            "Test 0.6923230578611185\n",
            "0.6929776634939768\n",
            "Epoch: 3\n",
            "Train:  1.0368056045128748\n",
            "Test 0.6910057216336876\n",
            "0.6923096615087839\n",
            "Epoch: 4\n",
            "Train:  1.0349710970134525\n",
            "Test 0.6893630200253301\n",
            "0.6914331022859574\n",
            "Epoch: 5\n",
            "Train:  1.0319686974142934\n",
            "Test 0.6868401909922506\n",
            "0.6901881679644349\n",
            "Epoch: 6\n",
            "Train:  1.0273199989682151\n",
            "Test 0.6823230615028968\n",
            "0.688197719340967\n",
            "Epoch: 7\n",
            "Train:  1.0202072184164446\n",
            "Test 0.6757242849894932\n",
            "0.6852001185033229\n",
            "Epoch: 8\n",
            "Train:  1.0096163358880488\n",
            "Test 0.66621842838469\n",
            "0.6808152539172684\n",
            "Epoch: 9\n",
            "Train:  0.993622633782062\n",
            "Test 0.6513033570387424\n",
            "0.6742028758532159\n",
            "Epoch: 10\n",
            "Train:  0.9688777064447438\n",
            "Test 0.6289453502103086\n",
            "0.6643007878411562\n",
            "Epoch: 11\n",
            "Train:  0.9320164541185121\n",
            "Test 0.5934231857677083\n",
            "0.6490792514076372\n",
            "Epoch: 12\n",
            "Train:  0.8752368329427181\n",
            "Test 0.5435261319190154\n",
            "0.6267405440875148\n",
            "Epoch: 13\n",
            "Train:  0.7952436817325516\n",
            "Test 0.4716436821462471\n",
            "0.5942941648491907\n",
            "Epoch: 14\n",
            "Train:  0.6923816995524661\n",
            "Test 0.3987859604341207\n",
            "0.5537665865722206\n",
            "Epoch: 15\n",
            "Train:  0.5900193331114975\n",
            "Test 0.3299851880405412\n",
            "0.507714043003024\n",
            "Epoch: 16\n",
            "Train:  0.5011715761252812\n",
            "Test 0.2799991532177715\n",
            "0.46112190333166186\n",
            "Epoch: 17\n",
            "Train:  0.4412728498515847\n",
            "Test 0.247694160147901\n",
            "0.4176532938408899\n",
            "Epoch: 18\n",
            "Train:  0.3986382148477621\n",
            "Test 0.22589402091808808\n",
            "0.37874064894252385\n",
            "Epoch: 19\n",
            "Train:  0.3655429353098293\n",
            "Test 0.20838326936239723\n",
            "0.344271773954364\n",
            "Epoch: 20\n",
            "Train:  0.3409658126778655\n",
            "Test 0.20252874009174743\n",
            "0.3156592633516433\n",
            "Epoch: 21\n",
            "Train:  0.3218743442301894\n",
            "Test 0.18731713338649317\n",
            "0.2898000298939991\n",
            "Epoch: 22\n",
            "Train:  0.3053899908049421\n",
            "Test 0.17783579354484877\n",
            "0.2672742136747779\n",
            "Epoch: 23\n",
            "Train:  0.29369759654469324\n",
            "Test 0.17855794980248688\n",
            "0.24944677319291275\n",
            "Epoch: 24\n",
            "Train:  0.28127650237700225\n",
            "Test 0.16619174378913837\n",
            "0.23273262303817357\n",
            "Epoch: 25\n",
            "Train:  0.2692784165670147\n",
            "Test 0.16285523044921102\n",
            "0.21871477818393195\n",
            "Epoch: 26\n",
            "Train:  0.2625371277973656\n",
            "Test 0.17154633335687303\n",
            "0.20925822467511743\n",
            "Epoch: 27\n",
            "Train:  0.2632411103627402\n",
            "Test 0.15696795822049564\n",
            "0.19877990336310458\n",
            "Epoch: 28\n",
            "Train:  0.2511114759606074\n",
            "Test 0.16585088084578078\n",
            "0.19218389202647335\n",
            "Epoch: 29\n",
            "Train:  0.2548207085675154\n",
            "Test 0.1546474306438214\n",
            "0.18466729465309525\n",
            "Epoch: 30\n",
            "Train:  0.24367575734280622\n",
            "Test 0.15202713591275197\n",
            "0.17813279144650157\n",
            "Epoch: 31\n",
            "Train:  0.24065883943449445\n",
            "Test 0.15309360186695617\n",
            "0.1731209827666667\n",
            "Epoch: 32\n",
            "Train:  0.24016319762492355\n",
            "Test 0.15700789235341242\n",
            "0.16989632081168446\n",
            "Epoch: 33\n",
            "Train:  0.23890076877869965\n",
            "Test 0.14919219041864076\n",
            "0.16575339401962186\n",
            "Epoch: 34\n",
            "Train:  0.2327945553817046\n",
            "Test 0.14808023171055884\n",
            "0.16221732715868006\n",
            "Epoch: 35\n",
            "Train:  0.23008414763861743\n",
            "Test 0.1512294760171747\n",
            "0.16001904354656069\n",
            "Epoch: 36\n",
            "Train:  0.22902151687077552\n",
            "Test 0.14680787542964513\n",
            "0.157376123781978\n",
            "Epoch: 37\n",
            "Train:  0.22455146811875232\n",
            "Test 0.15414000605488873\n",
            "0.1567287657855645\n",
            "Epoch: 38\n",
            "Train:  0.22803362243341438\n",
            "Test 0.1453077162785674\n",
            "0.1544441762916258\n",
            "Epoch: 39\n",
            "Train:  0.22334203591221438\n",
            "Test 0.14433343566599346\n",
            "0.15242175934117635\n",
            "Epoch: 40\n",
            "Train:  0.22129818838048768\n",
            "Test 0.1445173966312856\n",
            "0.15084071867411683\n",
            "Epoch: 41\n",
            "Train:  0.21958493041011057\n",
            "Test 0.1602951711514494\n",
            "0.15273177004244223\n",
            "Epoch: 42\n",
            "Train:  0.2289996481203771\n",
            "Test 0.14446745333244732\n",
            "0.15107879420473716\n",
            "Epoch: 43\n",
            "Train:  0.21362569339013907\n",
            "Test 0.14741644482281838\n",
            "0.1503462844467287\n",
            "Epoch: 44\n",
            "Train:  0.21563562186223356\n",
            "Test 0.15032788123184945\n",
            "0.15034260364343127\n",
            "Epoch: 45\n",
            "Train:  0.21552364466773286\n",
            "Test 0.1415099505976443\n",
            "0.1485760114775211\n",
            "Epoch: 46\n",
            "Train:  0.20820023711675253\n",
            "Test 0.14681717148221238\n",
            "0.14822423367232535\n",
            "Epoch: 47\n",
            "Train:  0.20858169322826595\n",
            "Test 0.14207646119725573\n",
            "0.14699465175671844\n",
            "Epoch: 48\n",
            "Train:  0.20479659834405878\n",
            "Test 0.14069663417515355\n",
            "0.14573502576792688\n",
            "Epoch: 49\n",
            "Train:  0.20382770452897436\n",
            "Test 0.14114897714538888\n",
            "0.1448178029523778\n",
            "Epoch: 50\n",
            "Train:  0.20439008523747598\n",
            "Test 0.14347300868849833\n",
            "0.14454884102859963\n",
            "Epoch: 51\n",
            "Train:  0.2020931101127804\n",
            "Test 0.13810951369490487\n",
            "0.1432609637978939\n",
            "Epoch: 52\n",
            "Train:  0.19828532570495438\n",
            "Test 0.1446349173238426\n",
            "0.14353575651113368\n",
            "Epoch: 53\n",
            "Train:  0.19901627000598682\n",
            "Test 0.14602162055807671\n",
            "0.14403293222701477\n",
            "Epoch: 54\n",
            "Train:  0.20300673569242159\n",
            "Test 0.13868125058000996\n",
            "0.14296259089183583\n",
            "Epoch: 55\n",
            "Train:  0.19507375709889901\n",
            "Test 0.13924921657071337\n",
            "0.14221991324892438\n",
            "Epoch: 56\n",
            "Train:  0.19347015022378647\n",
            "Test 0.13644058515928387\n",
            "0.14106404417130056\n",
            "Epoch: 57\n",
            "Train:  0.18785061031211536\n",
            "Test 0.141331779473758\n",
            "0.1411175913600121\n",
            "Epoch: 58\n",
            "Train:  0.1915237101531782\n",
            "Test 0.13805159899122985\n",
            "0.14050439171159848\n",
            "Epoch: 59\n",
            "Train:  0.18678378619444677\n",
            "Test 0.13479646300767367\n",
            "0.1393628042213358\n",
            "Epoch: 60\n",
            "Train:  0.18336714872401275\n",
            "Test 0.13479027358112317\n",
            "0.1384482969721106\n",
            "Epoch: 61\n",
            "Train:  0.18093905543683322\n",
            "Test 0.14080558949228608\n",
            "0.13891975593855133\n",
            "Epoch: 62\n",
            "Train:  0.18253254212589162\n",
            "Test 0.13349618503462263\n",
            "0.13783504090665727\n",
            "Epoch: 63\n",
            "Train:  0.17873016340937806\n",
            "Test 0.13331923281753455\n",
            "0.13693187872190862\n",
            "Epoch: 64\n",
            "Train:  0.17652897657519515\n",
            "Test 0.13855575030539935\n",
            "0.13725665320169816\n",
            "Epoch: 65\n",
            "Train:  0.17769743604483185\n",
            "Test 0.1365003609286123\n",
            "0.13710539468631522\n",
            "Epoch: 66\n",
            "Train:  0.17545906680844206\n",
            "Test 0.13302235223434783\n",
            "0.1362887859334738\n",
            "Epoch: 67\n",
            "Train:  0.17219826950258388\n",
            "Test 0.13279491468441873\n",
            "0.13559001150400082\n",
            "Epoch: 68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:26:35,863]\u001b[0m Trial 6 finished with value: 0.13651712715373315 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 384, 'layer_size4': 256, 'layer_size5': 128, 'learning_rate': 1.136242288042399e-05, 'b1': 0.9402284359422156}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.17003104733021746\n",
            "Test 0.1402257291989012\n",
            "0.1365171552336833\n",
            "Epoch: 0\n",
            "Train:  0.6927683366523995\n",
            "Test 0.6938792589383248\n",
            "0.6938792589383248\n",
            "Epoch: 1\n",
            "Train:  1.0388577724551107\n",
            "Test 0.6929541164702111\n",
            "0.6933652909004839\n",
            "Epoch: 2\n",
            "Train:  1.037424767301196\n",
            "Test 0.6916714483128362\n",
            "0.692671093118661\n",
            "Epoch: 3\n",
            "Train:  1.0353029322274876\n",
            "Test 0.6897457562960111\n",
            "0.6916801253603108\n",
            "Epoch: 4\n",
            "Train:  1.032039524653019\n",
            "Test 0.6865890283724327\n",
            "0.6901656391073723\n",
            "Epoch: 5\n",
            "Train:  1.026767910181821\n",
            "Test 0.6815776294404334\n",
            "0.6878378110035313\n",
            "Epoch: 6\n",
            "Train:  1.0181985406866878\n",
            "Test 0.6744121564176929\n",
            "0.6844401359917284\n",
            "Epoch: 7\n",
            "Train:  1.0053772241859646\n",
            "Test 0.6616722384651939\n",
            "0.6789685820656941\n",
            "Epoch: 8\n",
            "Train:  0.9847725677839566\n",
            "Test 0.6418637725023123\n",
            "0.6703971869786435\n",
            "Epoch: 9\n",
            "Train:  0.9515515018950452\n",
            "Test 0.6097516205721286\n",
            "0.6568090596332855\n",
            "Epoch: 10\n",
            "Train:  0.9000986349451673\n",
            "Test 0.5639606864897759\n",
            "0.6364943661191546\n",
            "Epoch: 11\n",
            "Train:  0.8307857250337636\n",
            "Test 0.5054690726510771\n",
            "0.608355628070848\n",
            "Epoch: 12\n",
            "Train:  0.744673280052213\n",
            "Test 0.4409396013060769\n",
            "0.572924580265585\n",
            "Epoch: 13\n",
            "Train:  0.6555185668416076\n",
            "Test 0.3817784297160613\n",
            "0.5329366629534718\n",
            "Epoch: 14\n",
            "Train:  0.5793965415203528\n",
            "Test 0.3359762933446374\n",
            "0.4921080602834974\n",
            "Epoch: 15\n",
            "Train:  0.5241578562464906\n",
            "Test 0.3049374764218872\n",
            "0.4535897494466004\n",
            "Epoch: 16\n",
            "Train:  0.4830618735962298\n",
            "Test 0.27867535793737613\n",
            "0.41780097969414964\n",
            "Epoch: 17\n",
            "Train:  0.4486639265503202\n",
            "Test 0.2582769214888632\n",
            "0.3853108784211471\n",
            "Epoch: 18\n",
            "Train:  0.4196801004392324\n",
            "Test 0.2434438107869564\n",
            "0.35652258181635055\n",
            "Epoch: 19\n",
            "Train:  0.39755999648964013\n",
            "Test 0.22788141236637102\n",
            "0.3304942618279202\n",
            "Epoch: 20\n",
            "Train:  0.3724509766905299\n",
            "Test 0.21872426187380767\n",
            "0.30793216320759015\n",
            "Epoch: 21\n",
            "Train:  0.3563322134234093\n",
            "Test 0.20369438592330877\n",
            "0.2869296364563816\n",
            "Epoch: 22\n",
            "Train:  0.3348232539636748\n",
            "Test 0.1937457764923791\n",
            "0.268182199126473\n",
            "Epoch: 23\n",
            "Train:  0.3191270835206404\n",
            "Test 0.18432524767550792\n",
            "0.25133123239578703\n",
            "Epoch: 24\n",
            "Train:  0.3034340500449523\n",
            "Test 0.178315995486228\n",
            "0.2366728070486344\n",
            "Epoch: 25\n",
            "Train:  0.2915044754416078\n",
            "Test 0.17273700959134453\n",
            "0.22384688358207858\n",
            "Epoch: 26\n",
            "Train:  0.2819203796144916\n",
            "Test 0.17106156471448067\n",
            "0.21326423252836138\n",
            "Epoch: 27\n",
            "Train:  0.27347752408230264\n",
            "Test 0.16356041066812507\n",
            "0.20330420265667248\n",
            "Epoch: 28\n",
            "Train:  0.26533876360827313\n",
            "Test 0.16478718582527105\n",
            "0.19558886037648543\n",
            "Epoch: 29\n",
            "Train:  0.2655123389247573\n",
            "Test 0.1568166387605143\n",
            "0.18782480461780024\n",
            "Epoch: 30\n",
            "Train:  0.25613457433906667\n",
            "Test 0.1581822054344656\n",
            "0.1818904076390301\n",
            "Epoch: 31\n",
            "Train:  0.25357262420403215\n",
            "Test 0.1538849215367775\n",
            "0.1762848692535177\n",
            "Epoch: 32\n",
            "Train:  0.2478094566818122\n",
            "Test 0.15569712765405685\n",
            "0.17216470947211557\n",
            "Epoch: 33\n",
            "Train:  0.24897616546628348\n",
            "Test 0.15093858709265462\n",
            "0.16791733131963762\n",
            "Epoch: 34\n",
            "Train:  0.24139780338321412\n",
            "Test 0.1507938626757908\n",
            "0.16449124780628518\n",
            "Epoch: 35\n",
            "Train:  0.24060141209226388\n",
            "Test 0.1536774055097566\n",
            "0.16232777726064712\n",
            "Epoch: 36\n",
            "Train:  0.23961861621274616\n",
            "Test 0.14850788020388983\n",
            "0.1595630800928747\n",
            "Epoch: 37\n",
            "Train:  0.23505709284827822\n",
            "Test 0.1470736294793777\n",
            "0.1570646710709227\n",
            "Epoch: 38\n",
            "Train:  0.23079621593293909\n",
            "Test 0.14607610138672175\n",
            "0.15486659191553873\n",
            "Epoch: 39\n",
            "Train:  0.22847732359836825\n",
            "Test 0.146982240152883\n",
            "0.15328951193312115\n",
            "Epoch: 40\n",
            "Train:  0.22881943407731178\n",
            "Test 0.14492578154955155\n",
            "0.15161658796061692\n",
            "Epoch: 41\n",
            "Train:  0.22400981668150904\n",
            "Test 0.14698009336906256\n",
            "0.15068921014972692\n",
            "Epoch: 42\n",
            "Train:  0.22758308969670055\n",
            "Test 0.14905854125571993\n",
            "0.15036305417390003\n",
            "Epoch: 43\n",
            "Train:  0.22213668379959933\n",
            "Test 0.14733947218556107\n",
            "0.14975830485054725\n",
            "Epoch: 44\n",
            "Train:  0.2237951968397413\n",
            "Test 0.14244922221853182\n",
            "0.14829642465028114\n",
            "Epoch: 45\n",
            "Train:  0.21842283442370838\n",
            "Test 0.1427640209568071\n",
            "0.14718990535501622\n",
            "Epoch: 46\n",
            "Train:  0.21642346598972112\n",
            "Test 0.1416398183657573\n",
            "0.1460798570135329\n",
            "Epoch: 47\n",
            "Train:  0.21373508832393548\n",
            "Test 0.14375192920366922\n",
            "0.14561426106842365\n",
            "Epoch: 48\n",
            "Train:  0.2150782461895611\n",
            "Test 0.14098657029015677\n",
            "0.14468870640032322\n",
            "Epoch: 49\n",
            "Train:  0.21061197894833464\n",
            "Test 0.13953779920280635\n",
            "0.14365851025736917\n",
            "Epoch: 50\n",
            "Train:  0.20826218307236613\n",
            "Test 0.13815592867987497\n",
            "0.14255798137605186\n",
            "Epoch: 51\n",
            "Train:  0.20839276336706603\n",
            "Test 0.14057222076933898\n",
            "0.14216082562693566\n",
            "Epoch: 52\n",
            "Train:  0.207790000275487\n",
            "Test 0.13698429204080076\n",
            "0.14112551134414109\n",
            "Epoch: 53\n",
            "Train:  0.20325545198966377\n",
            "Test 0.13604883953328534\n",
            "0.1401101710462839\n",
            "Epoch: 54\n",
            "Train:  0.2015543033184668\n",
            "Test 0.13862524397207268\n",
            "0.13981318424249223\n",
            "Epoch: 55\n",
            "Train:  0.20396566357388546\n",
            "Test 0.14058650091577035\n",
            "0.13996784815581426\n",
            "Epoch: 56\n",
            "Train:  0.19997957609627984\n",
            "Test 0.13825259816017507\n",
            "0.1396247971298814\n",
            "Epoch: 57\n",
            "Train:  0.19776336669891173\n",
            "Test 0.13383030554538955\n",
            "0.13846589603796625\n",
            "Epoch: 58\n",
            "Train:  0.19163188238068732\n",
            "Test 0.15165848057758022\n",
            "0.14110441800029297\n",
            "Epoch: 59\n",
            "Train:  0.20444868603591626\n",
            "Test 0.1329443449557046\n",
            "0.13947240089031634\n",
            "Epoch: 60\n",
            "Train:  0.19033588079783398\n",
            "Test 0.13284947570317832\n",
            "0.1381478142289502\n",
            "Epoch: 61\n",
            "Train:  0.18786575290418805\n",
            "Test 0.13904247765903507\n",
            "0.13832674709046403\n",
            "Epoch: 62\n",
            "Train:  0.19118656592627803\n",
            "Test 0.13213449720320575\n",
            "0.13708829614127702\n",
            "Epoch: 63\n",
            "Train:  0.1867750320607462\n",
            "Test 0.13187037031729143\n",
            "0.13604471032141047\n",
            "Epoch: 64\n",
            "Train:  0.18691313165093298\n",
            "Test 0.13414830114306958\n",
            "0.13566542829527894\n",
            "Epoch: 65\n",
            "Train:  0.19075693682837386\n",
            "Test 0.14224517410928075\n",
            "0.1369813779867417\n",
            "Epoch: 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:27:10,125]\u001b[0m Trial 7 finished with value: 0.1370246903618498 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 128, 'layer_size5': 192, 'learning_rate': 1.0030868786434032e-05, 'b1': 0.9019054426647509}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.18642123758574544\n",
            "Test 0.1371981599828699\n",
            "0.13702473439990154\n",
            "Epoch: 0\n",
            "Train:  0.693120056456262\n",
            "Test 0.6929522224398323\n",
            "0.6929522224398322\n",
            "Epoch: 1\n",
            "Train:  1.0394761505581083\n",
            "Test 0.6928776727491246\n",
            "0.6929108059449948\n",
            "Epoch: 2\n",
            "Train:  1.039330536301756\n",
            "Test 0.6928100970201877\n",
            "0.6928695317954836\n",
            "Epoch: 3\n",
            "Train:  1.0391794840494792\n",
            "Test 0.6927417638974313\n",
            "0.6928262499872002\n",
            "Epoch: 4\n",
            "Train:  1.0390733028506185\n",
            "Test 0.6926871022898635\n",
            "0.6927848566931329\n",
            "Epoch: 5\n",
            "Train:  1.038926702497643\n",
            "Test 0.6926230552432301\n",
            "0.6927409995042226\n",
            "Epoch: 6\n",
            "Train:  1.0387956566644676\n",
            "Test 0.6925544926535079\n",
            "0.6926937995958569\n",
            "Epoch: 7\n",
            "Train:  1.038660283708747\n",
            "Test 0.6924829133700975\n",
            "0.6926431196700905\n",
            "Epoch: 8\n",
            "Train:  1.0385252368319167\n",
            "Test 0.6924044629593036\n",
            "0.6925879887859168\n",
            "Epoch: 9\n",
            "Train:  1.0383701011156425\n",
            "Test 0.6923109716548151\n",
            "0.6925259208677276\n",
            "Epoch: 10\n",
            "Train:  1.0382143128049242\n",
            "Test 0.6922115292741265\n",
            "0.6924571337834612\n",
            "Epoch: 11\n",
            "Train:  1.038049237850385\n",
            "Test 0.6921058474006233\n",
            "0.6923816922008096\n",
            "Epoch: 12\n",
            "Train:  1.0378741111074175\n",
            "Test 0.6919956895021292\n",
            "0.6923000006188613\n",
            "Epoch: 13\n",
            "Train:  1.0376874268491627\n",
            "Test 0.6918798621757564\n",
            "0.6922121073430919\n",
            "Epoch: 14\n",
            "Train:  1.0374901180083935\n",
            "Test 0.6917603485313527\n",
            "0.692118460681767\n",
            "Epoch: 15\n",
            "Train:  1.037288711512045\n",
            "Test 0.6916338412752955\n",
            "0.6920187296206625\n",
            "Epoch: 16\n",
            "Train:  1.037093644067918\n",
            "Test 0.6915080294067606\n",
            "0.6919142366043352\n",
            "Epoch: 17\n",
            "Train:  1.0368900401688321\n",
            "Test 0.6913750304843916\n",
            "0.6918044170470778\n",
            "Epoch: 18\n",
            "Train:  1.0366723075672821\n",
            "Test 0.6912322511603107\n",
            "0.691688310599471\n",
            "Epoch: 19\n",
            "Train:  1.0364456735687815\n",
            "Test 0.6910797685057253\n",
            "0.6915651826116723\n",
            "Epoch: 20\n",
            "Train:  1.0362121373305828\n",
            "Test 0.6909185857563228\n",
            "0.6914346593762426\n",
            "Epoch: 21\n",
            "Train:  1.0359721644457442\n",
            "Test 0.6907569811894343\n",
            "0.6912981162282787\n",
            "Epoch: 22\n",
            "Train:  1.0357303217653826\n",
            "Test 0.6905817625286815\n",
            "0.6911539947452834\n",
            "Epoch: 23\n",
            "Train:  1.0354665975212614\n",
            "Test 0.6904019130455269\n",
            "0.6910028647139375\n",
            "Epoch: 24\n",
            "Train:  1.0351913625940734\n",
            "Test 0.6902089839453226\n",
            "0.6908434864461426\n",
            "Epoch: 25\n",
            "Train:  1.03489636523383\n",
            "Test 0.6900150751892901\n",
            "0.6906773019328999\n",
            "Epoch: 26\n",
            "Train:  1.0345889459817836\n",
            "Test 0.6898115650638119\n",
            "0.6905037348997467\n",
            "Epoch: 27\n",
            "Train:  1.034268360216539\n",
            "Test 0.6896003991256267\n",
            "0.6903227176065554\n",
            "Epoch: 28\n",
            "Train:  1.0339362710823505\n",
            "Test 0.6893940535220471\n",
            "0.6901366969366095\n",
            "Epoch: 29\n",
            "Train:  1.0336037867671841\n",
            "Test 0.6891724679059598\n",
            "0.6899436121030331\n",
            "Epoch: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:27:25,204]\u001b[0m Trial 8 finished with value: 0.6890591921100013 and parameters: {'layer_size1': 512, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 64, 'learning_rate': 1.9360183517613576e-06, 'b1': 0.9903978011454345}. Best is trial 0 with value: 0.1359454151202222.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  1.0332397523817125\n",
            "Test 0.6889379474269601\n",
            "0.6897422797779413\n",
            "Epoch: 0\n",
            "Train:  0.6931207885235657\n",
            "Test 0.6916176871065691\n",
            "0.6916176871065691\n",
            "Epoch: 1\n",
            "Train:  1.0369379841364348\n",
            "Test 0.6901710946044642\n",
            "0.6908140246053999\n",
            "Epoch: 2\n",
            "Train:  1.0337493806094913\n",
            "Test 0.6867474286984174\n",
            "0.6891473869386038\n",
            "Epoch: 3\n",
            "Train:  1.0264917013846038\n",
            "Test 0.6791440396955162\n",
            "0.6857587191733302\n",
            "Epoch: 4\n",
            "Train:  1.0110916936353886\n",
            "Test 0.6612237307614895\n",
            "0.6784601148147388\n",
            "Epoch: 5\n",
            "Train:  0.9761450000298328\n",
            "Test 0.6192325008657825\n",
            "0.6624061384429383\n",
            "Epoch: 6\n",
            "Train:  0.9003245003057487\n",
            "Test 0.543749172827263\n",
            "0.6323772259254066\n",
            "Epoch: 7\n",
            "Train:  0.7933690736800323\n",
            "Test 0.4670913495641925\n",
            "0.5926559216342129\n",
            "Epoch: 8\n",
            "Train:  0.6817188791848801\n",
            "Test 0.39762306486293947\n",
            "0.547602364157057\n",
            "Epoch: 9\n",
            "Train:  0.5884612797053306\n",
            "Test 0.330574458362637\n",
            "0.49897551480646524\n",
            "Epoch: 10\n",
            "Train:  0.507801925811248\n",
            "Test 0.2866052535154444\n",
            "0.4525101151060872\n",
            "Epoch: 11\n",
            "Train:  0.45363031833981854\n",
            "Test 0.25487925499786823\n",
            "0.41006729467308767\n",
            "Epoch: 12\n",
            "Train:  0.409483817034152\n",
            "Test 0.2355948940956549\n",
            "0.37314287300918275\n",
            "Epoch: 13\n",
            "Train:  0.3776722455417717\n",
            "Test 0.21277761666766015\n",
            "0.33959434173149317\n",
            "Epoch: 14\n",
            "Train:  0.3513093913321967\n",
            "Test 0.20683014261853563\n",
            "0.3120731873736616\n",
            "Epoch: 15\n",
            "Train:  0.32894294275032293\n",
            "Test 0.18382573296953908\n",
            "0.2856808173181661\n",
            "Epoch: 16\n",
            "Train:  0.29741150266303246\n",
            "Test 0.17830190594707215\n",
            "0.2637103030444664\n",
            "Epoch: 17\n",
            "Train:  0.29007886976494895\n",
            "Test 0.16647079865356068\n",
            "0.243905632946797\n",
            "Epoch: 18\n",
            "Train:  0.2697828308621169\n",
            "Test 0.16102537496404334\n",
            "0.22708720221950587\n",
            "Epoch: 19\n",
            "Train:  0.2607931946387221\n",
            "Test 0.15613431933816974\n",
            "0.2127311111824515\n",
            "Epoch: 20\n",
            "Train:  0.2539901173376775\n",
            "Test 0.15240428135508582\n",
            "0.20055342589502467\n",
            "Epoch: 21\n",
            "Train:  0.24395573942925466\n",
            "Test 0.1540355537591618\n",
            "0.191180692905158\n",
            "Epoch: 22\n",
            "Train:  0.24767350713166345\n",
            "Test 0.15593239238601683\n",
            "0.18408917184964838\n",
            "Epoch: 23\n",
            "Train:  0.2473644784893909\n",
            "Test 0.1488827511586331\n",
            "0.1770144784162991\n",
            "Epoch: 24\n",
            "Train:  0.23677631442336153\n",
            "Test 0.16458033023900165\n",
            "0.17451821817628765\n",
            "Epoch: 25\n",
            "Train:  0.24014098223830974\n",
            "Test 0.14484062729464783\n",
            "0.16856470661535122\n",
            "Epoch: 26\n",
            "Train:  0.22637472917827275\n",
            "Test 0.14563636476303632\n",
            "0.16396792390627993\n",
            "Epoch: 27\n",
            "Train:  0.2243310265215762\n",
            "Test 0.14220506558706472\n",
            "0.15960691682794645\n",
            "Epoch: 28\n",
            "Train:  0.21862957574727335\n",
            "Test 0.1454956260929396\n",
            "0.15678028467955404\n",
            "Epoch: 29\n",
            "Train:  0.2193681902692213\n",
            "Test 0.14045711773901415\n",
            "0.15351160486182408\n",
            "Epoch: 30\n",
            "Train:  0.21249187152641705\n",
            "Test 0.1397244304200232\n",
            "0.15075143643505537\n",
            "Epoch: 31\n",
            "Train:  0.21005345188463345\n",
            "Test 0.14361274002235888\n",
            "0.14932256508399994\n",
            "Epoch: 32\n",
            "Train:  0.2130589735704464\n",
            "Test 0.1377063814837199\n",
            "0.14699785490381628\n",
            "Epoch: 33\n",
            "Train:  0.20396321618267688\n",
            "Test 0.1370646675141194\n",
            "0.1450102095699567\n",
            "Epoch: 34\n",
            "Train:  0.2025945718219374\n",
            "Test 0.13651684172205872\n",
            "0.14331084665688373\n",
            "Epoch: 35\n",
            "Train:  0.19992270478091778\n",
            "Test 0.13683936023068077\n",
            "0.14201612921181048\n",
            "Epoch: 36\n",
            "Train:  0.19711676362096828\n",
            "Test 0.13564200543767804\n",
            "0.14074097340761016\n",
            "Epoch: 37\n",
            "Train:  0.19456307864287398\n",
            "Test 0.13610188908629364\n",
            "0.13981296380329275\n",
            "Epoch: 38\n",
            "Train:  0.19097953734598755\n",
            "Test 0.13269702317159035\n",
            "0.1383895391699682\n",
            "Epoch: 39\n",
            "Train:  0.18849749899650123\n",
            "Test 0.1373431246331978\n",
            "0.13818022844044597\n",
            "Epoch: 40\n",
            "Train:  0.18629165959199925\n",
            "Test 0.13170981217489575\n",
            "0.136886007562166\n",
            "Epoch: 41\n",
            "Train:  0.18317239447036288\n",
            "Test 0.13115153109634314\n",
            "0.1357390146936394\n",
            "Epoch: 42\n",
            "Train:  0.1818164138348548\n",
            "Test 0.1375404411565253\n",
            "0.13609932450763182\n",
            "Epoch: 43\n",
            "Train:  0.18108168114044945\n",
            "Test 0.12909533568715928\n",
            "0.13469845047270015\n",
            "Epoch: 44\n",
            "Train:  0.17702337869975185\n",
            "Test 0.12903920610199918\n",
            "0.13356655229744122\n",
            "Epoch: 45\n",
            "Train:  0.17239968226915534\n",
            "Test 0.13240087889953628\n",
            "0.1333334094940192\n",
            "Epoch: 46\n",
            "Train:  0.17487404312283455\n",
            "Test 0.13629370801587462\n",
            "0.13392548570306614\n",
            "Epoch: 47\n",
            "Train:  0.1793077366099089\n",
            "Test 0.13916328980590834\n",
            "0.13497306988554253\n",
            "Epoch: 48\n",
            "Train:  0.17462258440896106\n",
            "Test 0.1291778048657345\n",
            "0.13381399620301543\n",
            "Epoch: 49\n",
            "Train:  0.16918604700542958\n",
            "Test 0.12633434513877162\n",
            "0.1323180446392325\n",
            "Epoch: 50\n",
            "Train:  0.1697560735456236\n",
            "Test 0.14015593195518294\n",
            "0.13388564000119751\n",
            "Epoch: 51\n",
            "Train:  0.1718627512574387\n",
            "Test 0.1281532580067059\n",
            "0.13273915312984644\n",
            "Epoch: 52\n",
            "Train:  0.16048143586423588\n",
            "Test 0.1258467442659668\n",
            "0.13136066128373006\n",
            "Epoch: 53\n",
            "Train:  0.15703621317015026\n",
            "Test 0.12647881489861143\n",
            "0.13038428629881177\n",
            "Epoch: 54\n",
            "Train:  0.1607581734206978\n",
            "Test 0.12470043482405607\n",
            "0.1292475106873825\n",
            "Epoch: 55\n",
            "Train:  0.15632963639934763\n",
            "Test 0.12525761271434607\n",
            "0.12844952810716795\n",
            "Epoch: 56\n",
            "Train:  0.1527613071970396\n",
            "Test 0.1276086235227875\n",
            "0.12828134668689872\n",
            "Epoch: 57\n",
            "Train:  0.15453046674400076\n",
            "Test 0.12452441724119606\n",
            "0.12752995899854214\n",
            "Epoch: 58\n",
            "Train:  0.14885880994429682\n",
            "Test 0.12446549362178906\n",
            "0.1269170647491194\n",
            "Epoch: 59\n",
            "Train:  0.14478379819241963\n",
            "Test 0.13111290368405018\n",
            "0.12775623382212842\n",
            "Epoch: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-16 06:27:56,440]\u001b[0m Trial 9 finished with value: 0.12781817140888083 and parameters: {'layer_size1': 384, 'layer_size2': 512, 'layer_size3': 512, 'layer_size4': 64, 'layer_size5': 128, 'learning_rate': 1.8041786734871513e-05, 'b1': 0.942414121422847}. Best is trial 9 with value: 0.12781817140888083.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.15246355670057374\n",
            "Test 0.1280667048993251\n",
            "0.12781832811369512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial = study.best_trial"
      ],
      "metadata": {
        "id": "wzyqDmk-S37F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_trial.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGq-o8_5TJ79",
        "outputId": "d5b25547-1d09-45dd-e07e-0cb586d15afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'layer_size1': 384, 'layer_size2': 512, 'layer_size3': 512, 'layer_size4': 64, 'layer_size5': 128, 'learning_rate': 1.8041786734871513e-05, 'b1': 0.942414121422847}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "bRmBgX9dTNCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mydata.json', 'w') as f:\n",
        "    json.dump(best_trial.params, f)"
      ],
      "metadata": {
        "id": "rzTnkh0FTOsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(768,[384,256,512,64,192],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1.713644852336342e-05,betas=(0.9308401877808863,0.99))\n",
        "lossff = torch.nn.BCELoss()\n",
        "\n",
        "total_loss = 0\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "\n",
        "wloss = []\n",
        "\n",
        "for i in range(800):\n",
        "  print(\"Epoch:\", i)\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "  print(\"Train: \",total_loss / len(train_loader.dataset))\n",
        "\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  for data in val_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "\n",
        "  print(\"Test\", total_loss / len(val_loader.dataset))\n",
        "\n",
        "  weighted_loss = exp_param*(weighted_loss) + (1-exp_param)*(total_loss/ len(val_loader.dataset))\n",
        "  print(weighted_loss/(1-exp_param**(i+1)))\n",
        "  wloss.append(weighted_loss/(1-exp_param**(i+1)))\n",
        "\n",
        "  if(i-30>=0 and wloss[i-20]-weighted_loss<0.01):\n",
        "    break\n",
        "\n",
        "print(weighted_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TCJeYG3TUNd",
        "outputId": "24da9655-b566-42db-fc45-48c50f807a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train:  0.6925759520722833\n",
            "Test 0.6925863686935368\n",
            "0.6925863686935368\n",
            "Epoch: 1\n",
            "Train:  1.037886774801946\n",
            "Test 0.6915017303093013\n",
            "0.6919837918134061\n",
            "Epoch: 2\n",
            "Train:  1.035775717034008\n",
            "Test 0.6895549251483037\n",
            "0.6909883546555773\n",
            "Epoch: 3\n",
            "Train:  1.031951198101917\n",
            "Test 0.6858244481540862\n",
            "0.689239063835289\n",
            "Epoch: 4\n",
            "Train:  1.02443078487784\n",
            "Test 0.6786982467322996\n",
            "0.6861034090569128\n",
            "Epoch: 5\n",
            "Train:  1.0106353102586207\n",
            "Test 0.6640936345844478\n",
            "0.6801375364550867\n",
            "Epoch: 6\n",
            "Train:  0.9833837880756392\n",
            "Test 0.6345520486761799\n",
            "0.6686010656083978\n",
            "Epoch: 7\n",
            "Train:  0.9306704917233506\n",
            "Test 0.5799172134189815\n",
            "0.647288668227071\n",
            "Epoch: 8\n",
            "Train:  0.8403752478924427\n",
            "Test 0.49931158742188536\n",
            "0.6131052284429896\n",
            "Epoch: 9\n",
            "Train:  0.7190413482132412\n",
            "Test 0.4095353399004255\n",
            "0.567493756193643\n",
            "Epoch: 10\n",
            "Train:  0.5885355036992294\n",
            "Test 0.32235305715393237\n",
            "0.5138583719620381\n",
            "Epoch: 11\n",
            "Train:  0.47343233435145227\n",
            "Test 0.2591274885030893\n",
            "0.4591528611958181\n",
            "Epoch: 12\n",
            "Train:  0.38723551812189405\n",
            "Test 0.21385250750915472\n",
            "0.40723878387554\n",
            "Epoch: 13\n",
            "Train:  0.3300717247602267\n",
            "Test 0.1870119096813621\n",
            "0.3611671574793793\n",
            "Epoch: 14\n",
            "Train:  0.29456928334175014\n",
            "Test 0.17118514991013994\n",
            "0.32178512384036134\n",
            "Epoch: 15\n",
            "Train:  0.27231474596693184\n",
            "Test 0.16109653182955452\n",
            "0.2887166095080455\n",
            "Epoch: 16\n",
            "Train:  0.2586167125038175\n",
            "Test 0.1555627446362387\n",
            "0.26147235035687827\n",
            "Epoch: 17\n",
            "Train:  0.25085583861385075\n",
            "Test 0.1523887322935866\n",
            "0.23925540177887106\n",
            "Epoch: 18\n",
            "Train:  0.24575925904970902\n",
            "Test 0.15943573759152338\n",
            "0.2230580403627194\n",
            "Epoch: 19\n",
            "Train:  0.2463191094647943\n",
            "Test 0.15275484121544458\n",
            "0.20883340161417985\n",
            "Epoch: 20\n",
            "Train:  0.23953958044012824\n",
            "Test 0.14833392373426058\n",
            "0.19662086527199843\n",
            "Epoch: 21\n",
            "Train:  0.2385797012570031\n",
            "Test 0.14666443612876828\n",
            "0.18655530874526918\n",
            "Epoch: 22\n",
            "Train:  0.23134719707317405\n",
            "Test 0.14521514924563766\n",
            "0.178238181189841\n",
            "Epoch: 23\n",
            "Train:  0.2265928943979216\n",
            "Test 0.145344238114226\n",
            "0.1716281777158608\n",
            "Epoch: 24\n",
            "Train:  0.2356073134902851\n",
            "Test 0.14229139399070007\n",
            "0.16573857066447928\n",
            "Epoch: 25\n",
            "Train:  0.22224776760671602\n",
            "Test 0.1410349697563729\n",
            "0.160782872805117\n",
            "Epoch: 26\n",
            "Train:  0.21906113158740895\n",
            "Test 0.14026206851878883\n",
            "0.15666876464469173\n",
            "Epoch: 27\n",
            "Train:  0.2182937649116193\n",
            "Test 0.1395259549220403\n",
            "0.15323355804423344\n",
            "Epoch: 28\n",
            "Train:  0.214392043336298\n",
            "Test 0.1387513056124523\n",
            "0.15033261857147112\n",
            "Epoch: 29\n",
            "Train:  0.21062552464968992\n",
            "Test 0.13853564698440143\n",
            "0.14797029984512025\n",
            "Epoch: 30\n",
            "Train:  0.20930815150197118\n",
            "Test 0.13672019961552742\n",
            "0.14571804927827753\n",
            "Epoch: 31\n",
            "Train:  0.20650828208078395\n",
            "Test 0.14080413185305649\n",
            "0.1447344865345443\n",
            "Epoch: 32\n",
            "Train:  0.20868885190986888\n",
            "Test 0.1347662407099764\n",
            "0.1427395729429248\n",
            "Epoch: 33\n",
            "Train:  0.20381273815729023\n",
            "Test 0.13401641531577912\n",
            "0.14099405633542558\n",
            "Epoch: 34\n",
            "Train:  0.20014605508100636\n",
            "Test 0.133478914643382\n",
            "0.1394904180488644\n",
            "Epoch: 35\n",
            "Train:  0.19734440258134417\n",
            "Test 0.13317042253502123\n",
            "0.13822600862177725\n",
            "Epoch: 36\n",
            "Train:  0.19466495613530005\n",
            "Test 0.1333443283925563\n",
            "0.13724941903878185\n",
            "Epoch: 37\n",
            "Train:  0.19135897886349168\n",
            "Test 0.13457853606332354\n",
            "0.13671513147650494\n",
            "Epoch: 38\n",
            "Train:  0.1909915565243571\n",
            "Test 0.13009204320445822\n",
            "0.13539029369566208\n",
            "Epoch: 39\n",
            "Train:  0.1894540201988576\n",
            "Test 0.13673095482882563\n",
            "0.1356584615679191\n",
            "Epoch: 40\n",
            "Train:  0.19242469660064457\n",
            "Test 0.12974374034465888\n",
            "0.13447539151768054\n",
            "Epoch: 41\n",
            "Train:  0.18367566665013632\n",
            "Test 0.13477432804229933\n",
            "0.13453518390917843\n",
            "Epoch: 42\n",
            "Train:  0.1863003771602016\n",
            "Test 0.13092950385937216\n",
            "0.1338139988179032\n",
            "Epoch: 43\n",
            "Train:  0.1787202818704694\n",
            "Test 0.1369999108440328\n",
            "0.13445121591652795\n",
            "Epoch: 44\n",
            "Train:  0.19292481511237694\n",
            "Test 0.12595068174840768\n",
            "0.13275103502958213\n",
            "Epoch: 45\n",
            "Train:  0.1769002217137606\n",
            "Test 0.12677731096144124\n",
            "0.13155624858372253\n",
            "Epoch: 46\n",
            "Train:  0.17270246342561402\n",
            "Test 0.12547014705536566\n",
            "0.1303389943459554\n",
            "Epoch: 47\n",
            "Train:  0.1695272055328329\n",
            "Test 0.1245537461515093\n",
            "0.12918191890342157\n",
            "Epoch: 48\n",
            "Train:  0.1757595618306608\n",
            "Test 0.12702089690026783\n",
            "0.1287497067918691\n",
            "Epoch: 49\n",
            "Train:  0.1672169429969875\n",
            "Test 0.1307034461738562\n",
            "0.12914046024528616\n",
            "Epoch: 50\n",
            "Train:  0.17424328916283127\n",
            "Test 0.1279462201950642\n",
            "0.12890160950804846\n",
            "Epoch: 51\n",
            "Train:  0.16797794180243333\n",
            "Test 0.12451198759860609\n",
            "0.12802367710678722\n",
            "Epoch: 52\n",
            "Train:  0.1606923097082765\n",
            "Test 0.13272653979286825\n",
            "0.12896425651729518\n",
            "Epoch: 53\n",
            "Train:  0.1657949399250822\n",
            "Test 0.1221658185861268\n",
            "0.1276045609822725\n",
            "Epoch: 54\n",
            "Train:  0.15698723360595904\n",
            "Test 0.12213899588191902\n",
            "0.12651144284990118\n",
            "Epoch: 55\n",
            "Train:  0.15591902263298796\n",
            "Test 0.12256366305817397\n",
            "0.1257218839374651\n",
            "Epoch: 56\n",
            "Train:  0.1542433485109018\n",
            "Test 0.12129871884613355\n",
            "0.12483724827134682\n",
            "Epoch: 57\n",
            "Train:  0.15134754590860708\n",
            "Test 0.12036744055880623\n",
            "0.12394328458822096\n",
            "Epoch: 58\n",
            "Train:  0.14926060548243233\n",
            "Test 0.12010808294509356\n",
            "0.1231762427902353\n",
            "Epoch: 59\n",
            "Train:  0.14931141338123508\n",
            "Test 0.12001781955688864\n",
            "0.12254455717551059\n",
            "Epoch: 60\n",
            "Train:  0.1458836030760855\n",
            "Test 0.12678794654925446\n",
            "0.12339323609073669\n",
            "Epoch: 61\n",
            "Train:  0.14663094873011331\n",
            "Test 0.12050723156212886\n",
            "0.12281603461889756\n",
            "Epoch: 62\n",
            "Train:  0.14237471084509576\n",
            "Test 0.12307042614880935\n",
            "0.122866912964801\n",
            "Epoch: 63\n",
            "Train:  0.14516737204277036\n",
            "Test 0.11922226546011565\n",
            "0.12213798300630717\n",
            "Epoch: 64\n",
            "Train:  0.14128252891175477\n",
            "Test 0.11853787013680944\n",
            "0.12141796007083505\n",
            "Epoch: 65\n",
            "Train:  0.1397991963413172\n",
            "Test 0.12457809425061467\n",
            "0.12204998716069808\n",
            "0.12204993812900615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "sJpEjGWITzlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def val(model):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in val_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(val_loader.dataset), accuracy, f1\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ],
      "metadata": {
        "id": "UzMcwdH0T0yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySeB3c2sT_mU",
        "outputId": "2d0c8756-07e0-432a-9717-dbf8629aa6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.04608704474491951, 0.9835164835164835, 0.984182776801406)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final = DataLoader(train_data_gos+val_data_gos, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "bF3gYQc2UEDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(768,[384,256,512,64,192],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1.713644852336342e-05,betas=(0.9308401877808863,0.99))\n",
        "lossff = torch.nn.BCELoss()\n",
        "\n",
        "total_loss = 0\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "\n",
        "wloss = []\n",
        "\n",
        "for i in range(70):\n",
        "  print(\"Epoch:\", i)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for data in final:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "  print(\"Train: \",total_loss / len(final.dataset))\n",
        "\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "\n",
        "  print(\"Test\", total_loss / len(test_loader.dataset))\n",
        "\n",
        "  weighted_loss = exp_param*(weighted_loss) + (1-exp_param)*(total_loss/ len(test_loader.dataset))\n",
        "  print(weighted_loss/(1-exp_param**(i+1)))\n",
        "  wloss.append(weighted_loss/(1-exp_param**(i+1)))\n",
        "\n",
        "  if(i-30>=0 and wloss[i-20]-weighted_loss<0.001):\n",
        "      break\n",
        "\n",
        "  print(weighted_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJMBWW4xUMGF",
        "outputId": "f77d6715-484e-4c40-d9e4-c244a1e07544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train:  0.6928040554817488\n",
            "Test 0.6921507486930261\n",
            "0.6921507486930261\n",
            "0.13843014973860518\n",
            "Epoch: 1\n",
            "Train:  0.691360590076563\n",
            "Test 0.6899105952336237\n",
            "0.6909062189933581\n",
            "0.24872623883760886\n",
            "Epoch: 2\n",
            "Train:  0.6876316991190042\n",
            "Test 0.6837955222025023\n",
            "0.6879919989971057\n",
            "0.33574009551058753\n",
            "Epoch: 3\n",
            "Train:  0.6770928546477004\n",
            "Test 0.6666947620692271\n",
            "0.6807774878426753\n",
            "0.40193102882231546\n",
            "Epoch: 4\n",
            "Train:  0.6461465028324989\n",
            "Test 0.6170685142824501\n",
            "0.6618255085589339\n",
            "0.4449585259143424\n",
            "Epoch: 5\n",
            "Train:  0.5650898526003073\n",
            "Test 0.5063463591393971\n",
            "0.6196820145927571\n",
            "0.45723609255935327\n",
            "Epoch: 6\n",
            "Train:  0.4412529022411258\n",
            "Test 0.4001550111141834\n",
            "0.5641255864598678\n",
            "0.44581987627031927\n",
            "Epoch: 7\n",
            "Train:  0.3566672228304021\n",
            "Test 0.3528291540486472\n",
            "0.5133470803211593\n",
            "0.4272217318259849\n",
            "Epoch: 8\n",
            "Train:  0.3165626852649181\n",
            "Test 0.3161587009936462\n",
            "0.4677955864399095\n",
            "0.4050091256595172\n",
            "Epoch: 9\n",
            "Train:  0.2832153856535971\n",
            "Test 0.28641652296750975\n",
            "0.4271561471819071\n",
            "0.38129060512111573\n",
            "Epoch: 10\n",
            "Train:  0.25333168367617587\n",
            "Test 0.25273549240150733\n",
            "0.38899390454442734\n",
            "0.3555795825771941\n",
            "Epoch: 11\n",
            "Train:  0.2229284078646929\n",
            "Test 0.22099603849016267\n",
            "0.35291500847443175\n",
            "0.3286628737597878\n",
            "Epoch: 12\n",
            "Train:  0.19870953292782637\n",
            "Test 0.1994123759614679\n",
            "0.32042851828647456\n",
            "0.30281277420012387\n",
            "Epoch: 13\n",
            "Train:  0.17930814420965857\n",
            "Test 0.18265219259283919\n",
            "0.2916056081542798\n",
            "0.2787806578786669\n",
            "Epoch: 14\n",
            "Train:  0.1674344397453598\n",
            "Test 0.17462559737565317\n",
            "0.2673564133040909\n",
            "0.2579496457780642\n",
            "Epoch: 15\n",
            "Train:  0.1648481806446781\n",
            "Test 0.16791692991560195\n",
            "0.2468925089255574\n",
            "0.23994310260557172\n",
            "Epoch: 16\n",
            "Train:  0.15872926709852814\n",
            "Test 0.16587154960239325\n",
            "0.23031502531588835\n",
            "0.22512879200493605\n",
            "Epoch: 17\n",
            "Train:  0.1604744681408481\n",
            "Test 0.16007417832388654\n",
            "0.21600914407172636\n",
            "0.21211786926872617\n",
            "Epoch: 18\n",
            "Train:  0.15699113927263042\n",
            "Test 0.16804909536908397\n",
            "0.20627687758975247\n",
            "0.2033041144887977\n",
            "Epoch: 19\n",
            "Train:  0.1484877068335611\n",
            "Test 0.15565372381236528\n",
            "0.19603415629784368\n",
            "0.19377403635351123\n",
            "Epoch: 20\n",
            "Train:  0.14898342663770195\n",
            "Test 0.16115726215335038\n",
            "0.18899384203120745\n",
            "0.18725068151347907\n",
            "Epoch: 21\n",
            "Train:  0.14579832659845532\n",
            "Test 0.15941805018610133\n",
            "0.18303471305130847\n",
            "0.18168415524800352\n",
            "Epoch: 22\n",
            "Train:  0.14485358467759957\n",
            "Test 0.1480943467536252\n",
            "0.17600514454338323\n",
            "0.17496619354912785\n",
            "Epoch: 23\n",
            "Train:  0.14012059026712753\n",
            "Test 0.14551616894019828\n",
            "0.1698784167688541\n",
            "0.16907618862734192\n",
            "Epoch: 24\n",
            "Train:  0.14235895406341087\n",
            "Test 0.1446227749243324\n",
            "0.16480813341099956\n",
            "0.16418550588674002\n",
            "Epoch: 25\n",
            "Train:  0.1406181641559825\n",
            "Test 0.1409931074036456\n",
            "0.160030689270596\n",
            "0.15954702619012112\n",
            "Epoch: 26\n",
            "Train:  0.14012709143770558\n",
            "Test 0.14682687828556085\n",
            "0.157383526627052\n",
            "0.15700299660920908\n",
            "Epoch: 27\n",
            "Train:  0.13393757510953042\n",
            "Test 0.13654440728736011\n",
            "0.1532076253913953\n",
            "0.15291127874483929\n",
            "Epoch: 28\n",
            "Train:  0.12992759100792609\n",
            "Test 0.13459940606748666\n",
            "0.1494802136362954\n",
            "0.14924890420936876\n",
            "Epoch: 29\n",
            "Train:  0.12665185488769568\n",
            "Test 0.1343539236556916\n",
            "0.14645120591023122\n",
            "0.14626990809863333\n",
            "Epoch: 30\n",
            "Train:  0.12481870696003183\n",
            "Test 0.13960492411703418\n",
            "0.14508059216148145\n",
            "0.1449369113023135\n",
            "Epoch: 31\n",
            "Train:  0.1259541550858153\n",
            "Test 0.12867089464660092\n",
            "0.14179605037640203\n",
            "0.141683707971171\n",
            "Epoch: 32\n",
            "Train:  0.12416257276466987\n",
            "Test 0.1251491700391193\n",
            "0.13846456272778676\n",
            "0.13837680038476066\n",
            "Epoch: 33\n",
            "Train:  0.11983574307336038\n",
            "Test 0.1244174178703364\n",
            "0.13565370848386893\n",
            "0.13558492388187582\n",
            "Epoch: 34\n",
            "Train:  0.12052464747166895\n",
            "Test 0.1222180738062649\n",
            "0.13296549107781744\n",
            "0.13291155386675363\n",
            "Epoch: 35\n",
            "Train:  0.11835699591908481\n",
            "Test 0.12393386642902325\n",
            "0.131158579771815\n",
            "0.13111601637920756\n",
            "Epoch: 36\n",
            "Train:  0.11502005282211798\n",
            "Test 0.12455331187545161\n",
            "0.12983718313836287\n",
            "0.12980347547845636\n",
            "Epoch: 37\n",
            "Train:  0.11741357138869381\n",
            "Test 0.11720539966509455\n",
            "0.12731030163095272\n",
            "0.127283860315784\n",
            "Epoch: 38\n",
            "Train:  0.10940751840129621\n",
            "Test 0.11272510585985777\n",
            "0.1243927777199261\n",
            "0.12437210942459875\n",
            "Epoch: 39\n",
            "Train:  0.10985726931374588\n",
            "Test 0.11081588743152199\n",
            "0.12167703867860898\n",
            "0.1216608650259834\n",
            "Epoch: 40\n",
            "Train:  0.10533225526339551\n",
            "Test 0.10892836740001655\n",
            "0.11912703325980313\n",
            "0.11911436550079003\n",
            "Epoch: 41\n",
            "Train:  0.10362187966758951\n",
            "Test 0.10583686151783982\n",
            "0.11646877277161778\n",
            "0.1164588647042\n",
            "Epoch: 42\n",
            "Train:  0.1045587037636538\n",
            "Test 0.10377344477498707\n",
            "0.1139295343606803\n",
            "0.11392178071835742\n",
            "Epoch: 43\n",
            "Train:  0.09914408265743792\n",
            "Test 0.10192870049849971\n",
            "0.11152923690362018\n",
            "0.11152316467438587\n",
            "Epoch: 44\n",
            "Train:  0.09850759412160869\n",
            "Test 0.09948848878398484\n",
            "0.10912098238541504\n",
            "0.10911622949630567\n",
            "Epoch: 45\n",
            "Train:  0.09549378048997004\n",
            "Test 0.09901846034261279\n",
            "0.10710040757009816\n",
            "0.1070966756655671\n",
            "Epoch: 46\n",
            "Train:  0.09300962449148097\n",
            "Test 0.09643860956169528\n",
            "0.10496798852525036\n",
            "0.10496506244479273\n",
            "Epoch: 47\n",
            "Train:  0.09068612193504533\n",
            "Test 0.10228988836645643\n",
            "0.10443235654849933\n",
            "0.10443002762912547\n",
            "Epoch: 48\n",
            "Train:  0.09066788665836584\n",
            "Test 0.09115558453313596\n",
            "0.10177695477147593\n",
            "0.10177513900992757\n",
            "Epoch: 49\n",
            "Train:  0.08954743562454996\n",
            "Test 0.08960772852921661\n",
            "0.09934307478552812\n",
            "0.09934165691378538\n",
            "Epoch: 50\n",
            "Train:  0.0878013596939371\n",
            "Test 0.08847375623472445\n",
            "0.09716918625394827\n",
            "0.0971680767779732\n",
            "Epoch: 51\n",
            "Train:  0.08533796278693882\n",
            "Test 0.08565598976672133\n",
            "0.09486652592311635\n",
            "0.09486565937572283\n",
            "Epoch: 52\n",
            "Train:  0.08098619070440823\n",
            "Test 0.08366849243422568\n",
            "0.09262690285927434\n",
            "0.0926262259874234\n",
            "Epoch: 53\n",
            "Train:  0.07964874924102545\n",
            "Test 0.08163938693524826\n",
            "0.09042938682777601\n",
            "0.09042885817698837\n",
            "Epoch: 54\n",
            "Train:  0.07700574363471534\n",
            "Test 0.0802095848913458\n",
            "0.08838541688124063\n",
            "0.08838500351985987\n",
            "Epoch: 55\n",
            "Train:  0.07646481925678486\n",
            "Test 0.08856171483494642\n",
            "0.08842067660390406\n",
            "0.08842034578287716\n",
            "Epoch: 56\n",
            "Train:  0.07844245055820043\n",
            "Test 0.07607108251543054\n",
            "0.0859507503933365\n",
            "0.08595049312938784\n",
            "Epoch: 57\n",
            "Train:  0.07381877710676601\n",
            "Test 0.07898243192596968\n",
            "0.08455708336269359\n",
            "0.08455688088870421\n",
            "Epoch: 58\n",
            "Train:  0.07405710834509469\n",
            "Test 0.07275013284477995\n",
            "0.08219568873557744\n",
            "0.08219553127991935\n",
            "Epoch: 59\n",
            "Train:  0.07062335842049093\n",
            "Test 0.07178233445864239\n",
            "0.08011301468850175\n",
            "0.08011289191566397\n",
            "Epoch: 60\n",
            "Train:  0.06719971549730197\n",
            "Test 0.07026757788417977\n",
            "0.07814392491354032\n",
            "0.07814382910936714\n",
            "Epoch: 61\n",
            "Train:  0.0653133872960113\n",
            "Test 0.07130924324017196\n",
            "0.07677698723817808\n",
            "0.0767769119355281\n",
            "Epoch: 62\n",
            "Train:  0.06567190638919598\n",
            "Test 0.06648728898777576\n",
            "0.07471904597335928\n",
            "0.07471898734597764\n",
            "Epoch: 63\n",
            "Train:  0.06275227157881208\n",
            "Test 0.06376850933532466\n",
            "0.07252893727099884\n",
            "0.07252889174384704\n",
            "Epoch: 64\n",
            "Train:  0.060796155315688535\n",
            "Test 0.06494205519591113\n",
            "0.07101156009400282\n",
            "0.07101152443425986\n",
            "Epoch: 65\n",
            "Train:  0.059422604812159045\n",
            "Test 0.06297397465468982\n",
            "0.06940404236034488\n",
            "0.06940401447834585\n",
            "Epoch: 66\n",
            "Train:  0.056624442179743044\n",
            "Test 0.05830101859875214\n",
            "0.06718343689435124\n",
            "0.06718341530242711\n",
            "Epoch: 67\n",
            "Train:  0.054540079641377644\n",
            "Test 0.05616413255651983\n",
            "0.06497957546014996\n",
            "0.06497955875324565\n",
            "Epoch: 68\n",
            "Train:  0.05535515856170647\n",
            "Test 0.05518113787630539\n",
            "0.06301988754029662\n",
            "0.0630198745778576\n",
            "Epoch: 69\n",
            "Train:  0.050427803585490025\n",
            "Test 0.05736993404357752\n",
            "0.06188989665501228\n",
            "0.06188988647100158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8n8ktUR13kz",
        "outputId": "1d523404-ac86-4012-ce36-93f45090457f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.05736993974750186, 0.9871794871794872, 0.9869402985074628)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}